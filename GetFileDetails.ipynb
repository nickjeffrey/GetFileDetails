{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "694c3c91",
   "metadata": {
    "id": "694c3c91"
   },
   "source": [
    "# file server age report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217baad4",
   "metadata": {
    "id": "217baad4"
   },
   "source": [
    "This jupyter notebook will take CSV data showing details about files on a network share, and generate graphs based on file size and age.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e5aa4b",
   "metadata": {
    "id": "d1e5aa4b"
   },
   "source": [
    "# 1 - import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b105131f",
   "metadata": {
    "id": "b105131f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import matplotlib.pyplot as plt      #for creating graphs and charts\n",
    "import chardet                       #for file character set encoding detection (ascii, utf-16, utf-8, etc)\n",
    "import os                            #for deleting temporary files\n",
    "import shutil                        #for copying  temporary files\n",
    "from IPython.display import display  #for opening *.png files\n",
    "from PIL import Image                #for opening *.png files\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce12577f",
   "metadata": {
    "id": "ce12577f"
   },
   "source": [
    "# 2 - define CSV source data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dffcd8b",
   "metadata": {
    "id": "8dffcd8b"
   },
   "outputs": [],
   "source": [
    "#CSV_source_file = 'https://raw.githubusercontent.com/nickjeffrey/GetFileDetails/main/filenames.csv'\n",
    "#CSV_source_file = 'c:/temp/filenames.csv'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40318417",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "40318417",
    "outputId": "87f1ffee-e89a-4c37-dd5b-a18c9639b88c"
   },
   "outputs": [],
   "source": [
    "# determine default system character set\n",
    "import sys\n",
    "print(f\"The default system character set is:\", sys.stdout.encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3d9686-845a-4f53-b991-88f1ea0212d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect file encoding\n",
    "with open(CSV_source_file, \"rb\") as raw_file:\n",
    "    result = chardet.detect(raw_file.read(100000))  # Read a sample for detection\n",
    "    source_encoding = result.get(\"encoding\", \"\").lower()  # Convert to lowercase for consistency\n",
    "    #source_encoding = result[\"encoding\"]\n",
    "\n",
    "# Debug output\n",
    "if source_encoding:\n",
    "    print(f\"Source file is encoded as: {source_encoding}\")\n",
    "else:\n",
    "    print(\"WARNING: Unable to detect source file encoding.\")\n",
    "\n",
    "\n",
    "# Define temporary CSV filename\n",
    "CSV_source_file_temp = f\"{CSV_source_file}.tmp\"  # Define temporary CSV file name\n",
    "\n",
    "# Detect file encoding\n",
    "with open(CSV_source_file, \"rb\") as raw_file:\n",
    "    result = chardet.detect(raw_file.read(100000))  # Read a sample for detection\n",
    "    source_encoding = result.get(\"encoding\", \"\").lower()  # Convert to lowercase for consistency\n",
    "\n",
    "# Check if the file encoding needs conversion\n",
    "if source_encoding and source_encoding != \"utf-8\":\n",
    "    print(f\"Source file is encoded as {source_encoding}, attempting conversion to UTF-8\")\n",
    "\n",
    "    # Read file using detected encoding\n",
    "    try:\n",
    "        print(f\"Reading source file {CSV_source_file}\")\n",
    "        with open(CSV_source_file, \"r\", encoding=source_encoding, errors=\"replace\") as infile:\n",
    "            content = infile.read()\n",
    "\n",
    "        # Write to a temporary UTF-8 encoded file\n",
    "        print(f\"Writing temporary file {CSV_source_file_temp} encoded as UTF-8\")\n",
    "        with open(CSV_source_file_temp, \"w\", encoding=\"utf-8\") as outfile:\n",
    "            outfile.write(content)\n",
    "\n",
    "        print(f\"File converted from {source_encoding} to UTF-8 successfully.\")\n",
    "\n",
    "        # Replace original file with the UTF-8 encoded temp file\n",
    "        shutil.move(CSV_source_file_temp, CSV_source_file)\n",
    "        print(f\"Replaced original file {CSV_source_file} with UTF-8 encoded version.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to convert file due to: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"File is already UTF-8 encoded or encoding could not be detected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4eeba7-f3ad-4bbd-8bdf-107f6a8ce2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import chardet\n",
    "\n",
    "# Function to count lines in a CSV file\n",
    "def count_lines(file_path):\n",
    "    try:\n",
    "        # Detect file encoding to handle non-UTF-8 cases\n",
    "        with open(file_path, \"rb\") as raw_file:\n",
    "            result = chardet.detect(raw_file.read(100000))  # Read a sample for detection\n",
    "            detected_encoding = result.get(\"encoding\", \"utf-8\")  # Default to utf-8 if detection fails\n",
    "\n",
    "        print(f\"Detected encoding: {detected_encoding}\")\n",
    "\n",
    "        # Read file with detected encoding\n",
    "        line_count = 0\n",
    "        with open(file_path, 'r', encoding=detected_encoding, errors=\"replace\") as file:\n",
    "            for line in file:\n",
    "                line_count += 1\n",
    "                if line_count % 1000000 == 0:\n",
    "                    print(f\"Processed {line_count} lines\")\n",
    "\n",
    "        return line_count\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: File not found: {file_path}\")\n",
    "        return 0\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Unexpected issue while reading the file: {e}\")\n",
    "        return 0\n",
    "\n",
    "# Example usage:\n",
    "file_path = CSV_source_file          # Replace with actual file path\n",
    "num_lines = count_lines(file_path)   #call the function\n",
    "print(f\"Total number of lines in the file: {num_lines}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940acbcf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "940acbcf",
    "outputId": "ba5c4f21-7a22-4365-9087-60b1191408fd"
   },
   "outputs": [],
   "source": [
    "# extract all the lines from the source file that contain at least one comma, save to new temporary file\n",
    "# this is how we eliminate any bogus lines in the source file (ie blank lines, headers, etc)\n",
    "\n",
    "def find_lines_with_comma(input_file_path, output_file_path):\n",
    "    try:\n",
    "        with open(input_file_path, 'r', encoding='utf-8') as input_file:\n",
    "            lines = input_file.readlines()\n",
    "\n",
    "        matching_lines = [line.strip() for line in lines if ',' in line]\n",
    "\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "            for line in matching_lines:\n",
    "                output_file.write(line + '\\n')\n",
    "\n",
    "        print(f\"Matching lines have been written to a working copy of the file at '{output_file_path}'.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {input_file_path}\")\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"Unable to decode the file with 'utf-8' encoding: {input_file_path}\")\n",
    "        print(f\"The source CSV file may have a weird character set encoding\")\n",
    "        print(f\"Try to fix with these commands from the Windows command prompt\")\n",
    "        print(f\"type sourcefilename.csv | findstr , > sourcefilename.csv.tmp\")\n",
    "        print(f\"copy sourcefilename.csv.tmp sourcefilename.csv\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "input_file_path = CSV_source_file\n",
    "output_file_path = input_file_path   #start with name of source file\n",
    "output_file_path += \".tmp\"           #append .tmp to filename\n",
    "find_lines_with_comma(input_file_path, output_file_path)\n",
    "\n",
    "# Now make the cleaned up file the source file that all subsequent analysis will be performed on\n",
    "CSV_working_file = output_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66236b10",
   "metadata": {
    "id": "66236b10"
   },
   "outputs": [],
   "source": [
    "# BUG ALERT: some weird filenames that contain oddball characters like\n",
    "#    embedded quotation marks or multiple commas in the filenames may\n",
    "#    not be correctly detected by the pd.read_csv directive.\n",
    "#    Figure out which files were not matched, and come up with an \"exceptions list\" for further investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f633fde9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f633fde9",
    "outputId": "44c5e8a0-9536-43e7-81c3-fa9cfe829b92",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# figure out the character set encoding of the source file\n",
    "\n",
    "\n",
    "import chardet\n",
    "from chardet.universaldetector import UniversalDetector\n",
    "\n",
    "detector = UniversalDetector()\n",
    "for line in open(CSV_source_file,'rb'):\n",
    "    detector.feed(line)\n",
    "    if detector.done: break\n",
    "detector.close()\n",
    "print(detector.result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d02e11",
   "metadata": {
    "id": "99d02e11"
   },
   "source": [
    "# 2 - Load dataset into a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59961edc",
   "metadata": {
    "id": "59961edc"
   },
   "outputs": [],
   "source": [
    "# load the temporary working copy of the file \n",
    "\n",
    "df = pd.read_csv(CSV_working_file, on_bad_lines='skip',skip_blank_lines=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c751410",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "5c751410",
    "outputId": "e785cba2-bdd7-4a3b-bfea-f64b7cb41ace"
   },
   "outputs": [],
   "source": [
    "# look at the top few rows of the data to confirm the labels are correct\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c28cacf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "8c28cacf",
    "outputId": "e33aea56-715e-4842-9d0b-01cdaca7d4ae",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# look at the bottom few rows of the data\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbacb227",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bbacb227",
    "outputId": "fc0644e1-4b1a-4a9e-a037-34d4ac7c21c9"
   },
   "outputs": [],
   "source": [
    "# show number of rows in dataset\n",
    "print (\"Rows in dataset:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87269986",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "87269986",
    "outputId": "c8797fe1-baf6-41ad-ae5e-1bff176e8ac7"
   },
   "outputs": [],
   "source": [
    "#view dimensions of dataset (rows and columns)\n",
    "print (\"Rows,columns in dataset:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddde40a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "id": "0ddde40a",
    "outputId": "a276a811-4c74-4e33-e21e-4d304604d729"
   },
   "outputs": [],
   "source": [
    "# check to see if there are any missing values from the dataset\n",
    "\n",
    "# all of the results should be zero, which would indicate there are not any null values in the dataset\n",
    "# if there are any results greater than zero, it would indicate that some pieces of data are missing and should be cleaned up.\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Sg-SAwY7ingC",
   "metadata": {
    "id": "Sg-SAwY7ingC"
   },
   "outputs": [],
   "source": [
    "# Find rows with null values\n",
    "rows_with_nulls = df[df.isnull().any(axis=1)]\n",
    "\n",
    "if ( len(rows_with_nulls) > 0):\n",
    "   print(\"WARNING: Found \", len(rows_with_nulls), \" rows with null values\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qP4g4k39kks_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qP4g4k39kks_",
    "outputId": "f67ef4ce-7018-44d0-cc40-830e07572bec"
   },
   "outputs": [],
   "source": [
    "if ( len(rows_with_nulls) > 0):\n",
    "  print (rows_with_nulls)\n",
    "  print(f\"WARNING: {rows_with_nulls} rows containing null values were found, indicating problems with the source file.  Please investigate.\")\n",
    "else:\n",
    "  print(\"Looking good, there are no rows with null values, indicating a nice and clean input file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Kes1SZvAjInj",
   "metadata": {
    "id": "Kes1SZvAjInj"
   },
   "outputs": [],
   "source": [
    "# drop any rows containing null characters\n",
    "if ( len(rows_with_nulls) > 0):\n",
    "  print(\"Dropping \", len(rows_with_nulls), \" rows with null values\")\n",
    "  df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d367b92e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 577
    },
    "id": "d367b92e",
    "outputId": "dcdfce48-de7d-4ac9-f7ed-cb8e6d8184b7"
   },
   "outputs": [],
   "source": [
    "# visualize any missing values from the dataset in a histogram\n",
    "# you want all the bars in the graph to be empty, which would indicate zero missing values\n",
    "\n",
    "df.isnull().sum().plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b21a631",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3b21a631",
    "outputId": "4b363fe7-a986-4760-ed6f-c21768c838ab"
   },
   "outputs": [],
   "source": [
    "# another method to visualize missing values from dataset\n",
    "\n",
    "print (\"Checking for missing values in data set\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_nas(df: pd.DataFrame):\n",
    "    if df.isnull().sum().sum() != 0:\n",
    "        na_df = (df.isnull().sum() / len(df)) * 100\n",
    "        na_df = na_df.drop(na_df[na_df == 0].index).sort_values(ascending=False)\n",
    "        missing_data = pd.DataFrame({'Missing Ratio %' :na_df})\n",
    "        missing_data.plot(kind = \"barh\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('No NAs found')\n",
    "plot_nas(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d06631",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f8d06631",
    "outputId": "74bf760f-f7cb-4001-b60d-409aa39b561f"
   },
   "outputs": [],
   "source": [
    "#show the names of the columns (also called feature names)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b3e610",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c4b3e610",
    "outputId": "25ff5812-7291-4df7-cfdf-35ebb04af54a"
   },
   "outputs": [],
   "source": [
    "#show summary info about dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607d0ca7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "id": "607d0ca7",
    "outputId": "bbafdd88-16dd-4dc2-8f58-7a6d86c68b91"
   },
   "outputs": [],
   "source": [
    "# show data types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b851fc35",
   "metadata": {
    "id": "b851fc35"
   },
   "source": [
    "## 2.1 - rename features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb171850",
   "metadata": {
    "id": "bb171850"
   },
   "outputs": [],
   "source": [
    "# rename some features that have incorrect names\n",
    "\n",
    "if 'DaysSinceCreation'     in df.columns: df.rename(columns={'DaysSinceCreation'     : 'CreationTimeDays'     }, inplace=True)\n",
    "if 'DaysSinceAccess'       in df.columns: df.rename(columns={'DaysSinceAccess'       : 'AccessTimeDays'       }, inplace=True)\n",
    "if 'DaysSinceModification' in df.columns: df.rename(columns={'DaysSinceModification' : 'ModificationTimeDays' }, inplace=True)\n",
    "if 'LastWriteTimeEpoch'    in df.columns: df.rename(columns={'LastWriteTimeEpoch'    : 'ModificationTimeEpoch'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a8b980",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "93a8b980",
    "outputId": "5c1b546e-e8e1-42fa-9921-04282b1a4576"
   },
   "outputs": [],
   "source": [
    "# look at the top few rows of the data to confirm the labels are correct\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb718263",
   "metadata": {
    "id": "eb718263"
   },
   "source": [
    "## 2.2 - dimensionality reduction by removing features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd8cbbb",
   "metadata": {
    "id": "ebd8cbbb"
   },
   "source": [
    "In this example, we are dropping some of the columns from the dataset that are not useful.\n",
    "Please note that this is more of a \"data science\" exercise than a machine learning exercise, so it isn't that these features have no predictive value for a ML algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20eaf80",
   "metadata": {
    "id": "b20eaf80"
   },
   "outputs": [],
   "source": [
    "# drop any redundant columns from the dataset which does not have any predictive power.\n",
    "\n",
    "#In this example, we have features for bytes,MegaBytes, Gigabytes.\n",
    "# We really only need the bytes column, so get rid of the other two.\n",
    "if 'MegaBytes' in df.columns: df.drop('MegaBytes', axis=1, inplace=True)\n",
    "if 'GigaBytes' in df.columns: df.drop('GigaBytes', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# we really only care about the ModificationTimeDays and AccessTimeDays, so drop the other timestamps\n",
    "if 'CreationTimeEpoch'     in df.columns: df.drop('CreationTimeEpoch',     axis=1, inplace=True)\n",
    "if 'CreationTimeDays'      in df.columns: df.drop('CreationTimeDays',      axis=1, inplace=True)\n",
    "if 'AccessTimeEpoch'       in df.columns: df.drop('AccessTimeEpoch',       axis=1, inplace=True)\n",
    "if 'ModificationTimeEpoch' in df.columns: df.drop('ModificationTimeEpoch', axis=1, inplace=True)\n",
    "\n",
    "# There is a column for the MD5 checksum of each file, but at the moment, we only care about file age, so drop MD5sum\n",
    "if 'MD5sum' in df.columns: df.drop('MD5sum', axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84002d8c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "84002d8c",
    "outputId": "a2cad0c2-5aaa-4bcd-a2fd-8f2319adb178"
   },
   "outputs": [],
   "source": [
    "#Look at the dataset again, you should see several columns have been dropped\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bsQF9u4Sh65x",
   "metadata": {
    "id": "bsQF9u4Sh65x"
   },
   "outputs": [],
   "source": [
    "# Convert the 'bytes' and 'days' columns to integers\n",
    "# Note that we set bytes to a 64-bit integer to avoid overflow\n",
    "# import numpy as np\n",
    "\n",
    "df['Bytes'] = df['Bytes'].astype(np.int64)\n",
    "df['ModificationTimeDays'] = df['ModificationTimeDays'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t4LCyvAElYsd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t4LCyvAElYsd",
    "outputId": "b6c8761d-7ba5-45ca-84fb-b8729e2e56c7"
   },
   "outputs": [],
   "source": [
    "# the Bytes and ModificationTimeDays should be of type int32 or int64\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593b3497",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "593b3497",
    "outputId": "4c758478-ebb4-401d-acc8-7d46cf171ac4"
   },
   "outputs": [],
   "source": [
    "# At this point, we have 3 columns: Filename, Bytes, ModificationTimeDays\n",
    "# look at the top few rows of the data to confirm the labels are correct\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d20966",
   "metadata": {
    "id": "54d20966"
   },
   "outputs": [],
   "source": [
    "# drop rows to make the dataset a bit faster during testing\n",
    "# Let's say we want to drop all rows between indices 10000 and 999999 inclusive.\n",
    "# We can do this with the drop() function and the range() function to generate the indices:\n",
    "# df.drop(range(10000, 1000000), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d884b21c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d884b21c",
    "outputId": "c3e39621-400e-4c69-fbe6-2405d667e62c"
   },
   "outputs": [],
   "source": [
    "#view dimensions of dataset (rows and columns)\n",
    "print (\"Rows,columns in dataset:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j_emF0rAmPIi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "j_emF0rAmPIi",
    "outputId": "ab205b89-84bc-4dbb-dec6-214068f535bf"
   },
   "outputs": [],
   "source": [
    "# At this point, we have 4 columns: Filename, Bytes, AccessTimeDays, ModificationTimeDays\n",
    "# look at the top few rows of the data to confirm the labels are correct\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9379a7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "8c9379a7",
    "outputId": "0cb560f6-d95d-4aea-c663-c02639ae16df"
   },
   "outputs": [],
   "source": [
    "# At this point, we have 4 columns: Filename, Bytes, AccessTimeDays, ModificationTimeDays\n",
    "# look at the bottom few rows of the data to confirm the labels are correct\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac779fe-7ca9-45c0-9803-5ec68104f872",
   "metadata": {},
   "source": [
    "## 2.3 - Cleanup temporary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d9df31-f058-4708-a709-0490e79f3e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete temporary working copy of the CSV file\n",
    "import os\n",
    "from pathlib import Path\n",
    "if os.path.isfile(CSV_working_file):\n",
    "    os.remove(CSV_working_file)\n",
    "    print (\"Deleting temporary working copy \",CSV_working_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdcf628",
   "metadata": {
    "id": "dcdcf628"
   },
   "source": [
    "# 3 - Categorize files by Last Modification Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5910711d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5910711d",
    "outputId": "c3eef1c9-a8cf-43bb-b689-7da22769ee04"
   },
   "outputs": [],
   "source": [
    "# show a list of all the filenames with age 0 to  90 days old by extracting all rows with ModificationTimeDays <= 90\n",
    "LastModified_age0to90days = (df[(df['ModificationTimeDays'] >= 0) & (df['ModificationTimeDays'] < 90)])\n",
    "print(\"Number of files with last modification date   0 to  90  days: \", (len(LastModified_age0to90days)) )\n",
    "\n",
    "# show a list of all the filenames with age 90 to 180 days\n",
    "LastModified_age90to180days = (df[(df['ModificationTimeDays'] >= 90) & (df['ModificationTimeDays'] < 180)])\n",
    "print(\"Number of files with last modification date  90 to 180  days: \", (len(LastModified_age90to180days)) )\n",
    "\n",
    "# show a list of all the filenames with age 180 to 365 days\n",
    "LastModified_age180to365days = (df[(df['ModificationTimeDays'] >= 180) & (df['ModificationTimeDays'] < 365)])\n",
    "print(\"Number of files with last modification date 180 to 365  days: \", (len(LastModified_age180to365days)) )\n",
    "\n",
    "# show a list of all the filenames with age 1 to 2 years days\n",
    "LastModified_age1to2years = (df[(df['ModificationTimeDays'] >= (365*1)) & (df['ModificationTimeDays'] < (365*2))])\n",
    "print(\"Number of files with last modification date   1 to   2 years: \", (len(LastModified_age1to2years)) )\n",
    "\n",
    "# show a list of all the filenames with age 2 to 3 years days\n",
    "LastModified_age2to3years = (df[(df['ModificationTimeDays'] >= (365*2)) & (df['ModificationTimeDays'] < (365*3))])\n",
    "print(\"Number of files with last modification date   2 to   3 years: \", (len(LastModified_age2to3years)) )\n",
    "\n",
    "# show a list of all the filenames with age 3 to 5 years days\n",
    "LastModified_age3to5years = (df[(df['ModificationTimeDays'] >= (365*3)) & (df['ModificationTimeDays'] < (365*5))])\n",
    "print(\"Number of files with last modification date   3 to   5 years: \", (len(LastModified_age3to5years)) )\n",
    "\n",
    "# show a list of all the filenames with age 5 to 99 years days\n",
    "LastModified_age5to99years = (df[(df['ModificationTimeDays'] >= (365*5)) & (df['ModificationTimeDays'] < (365*99))])\n",
    "print(\"Number of files with last modification date   5 to  99 years: \", (len(LastModified_age5to99years)) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cedb28-7435-40d9-b066-4d6060d593cb",
   "metadata": {},
   "source": [
    "## 3.1 - Create graphs for file counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff1a23f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "6ff1a23f",
    "outputId": "4a7480ce-01bf-4450-95b8-387df5c6e87b"
   },
   "outputs": [],
   "source": [
    "# create a bar chart \n",
    "\n",
    "# function to add value labels\n",
    "def addlabels(x,y):\n",
    "    for i in range(len(x)):\n",
    "        plt.text(i, y[i], y[i], ha = 'center')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # creating data on which bar chart will be plot\n",
    "    x = \"0to90days\", \"90to180days\", \"180to365days\", \"1to2years\", \"2to3years\", \"3to5years\", \"5to99years\"\n",
    "    y = [len(LastModified_age0to90days), len(LastModified_age90to180days), len(LastModified_age180to365days), len(LastModified_age1to2years), len(LastModified_age2to3years), len(LastModified_age3to5years), len(LastModified_age5to99years)]\n",
    "\n",
    "    # setting figure size by using figure() function\n",
    "    plt.figure(figsize = (10, 5))\n",
    "\n",
    "    # making the bar chart on the data\n",
    "    plt.bar(x, y)\n",
    "\n",
    "    # calling the function to add value labels\n",
    "    addlabels(x, y)\n",
    "\n",
    "    # giving title to the plot\n",
    "    plt.title(\"Filename counts by Last Modification Date\")\n",
    "\n",
    "    # giving X and Y labels\n",
    "    plt.xlabel(\"Last Modification Date\")\n",
    "    plt.ylabel(\"Number of files\")\n",
    "\n",
    "    # Save the graph as a PNG file for future reference\n",
    "    graph_filename = CSV_source_file                                                    #start with name of source file\n",
    "    if CSV_source_file.lower().endswith(\".csv\"): graph_filename = CSV_source_file[:-4]  #remove the last 4 characters (.csv)\n",
    "    graph_filename += \"_BarChart\"                                                       #append the type of chart (bar, pie, etc) to the filename\n",
    "    graph_filename += \"_FileCount\"                                                      #append FileCount|ByteCount to the filename    \n",
    "    graph_filename += \"_LastModificationDate\"                                           #append LastModificationDate or LastAccessDate to the filename\n",
    "    graph_filename += \".png\"                                                            #append .png extension to the filename\n",
    "    plt.savefig(graph_filename, format=\"png\", dpi=300, bbox_inches=\"tight\")\n",
    "    print(f\"Graph saved as {graph_filename}\")\n",
    "\n",
    "    # visualizing the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504a12c4-3d9c-4195-a3af-93c5dd21190b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pie chart showing percentages\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "# Data for the pie chart\n",
    "labels = [\"0to90days\", \"90to180days\", \"180to365days\", \"1to2years\", \"2to3years\", \"3to5years\", \"5to99years\"]\n",
    "sizes = [len(LastModified_age0to90days), len(LastModified_age90to180days), len(LastModified_age180to365days), len(LastModified_age1to2years), len(LastModified_age2to3years), len(LastModified_age3to5years), len(LastModified_age5to99years)]\n",
    "\n",
    "# The chart gets cluttered and hard to read if there are labels on very small pie slices\n",
    "# Only put labels on pie slices larger than 10%\n",
    "# Function to format labels with percentage only if >10%\n",
    "def label_format(label, pct):\n",
    "    return f\"{label} \\n{pct:.1f}%\" if pct > 5 else \"\"  # Show label + % if >10%\n",
    "\n",
    "# Compute final labels with percentages for slices >5%\n",
    "total_size = sum(sizes)\n",
    "final_labels = [label_format(label, (size / total_size) * 100) for label, size in zip(labels, sizes)]\n",
    "\n",
    "# Set up figure size\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Create pie chart\n",
    "plt.pie(\n",
    "    sizes,\n",
    "    labels=final_labels,  # Labels include percentage only if >10%\n",
    "    labeldistance=0.5,    # Moves labels inside slices\n",
    ")\n",
    "\n",
    "# Set title\n",
    "plt.title(\"Percentage Filename Counts by Last Modification Date\")\n",
    "\n",
    "# Generate legend labels with percentages\n",
    "legend_labels = [f\"{label} ({(size / total_size) * 100:.1f}%)\" for label, size in zip(labels, sizes)]\n",
    "\n",
    "# Add legend with percentage values\n",
    "plt.legend(legend_labels, loc=\"best\", bbox_to_anchor=(1, 0.5), title=\"\")\n",
    "\n",
    "# Save the graph as a PNG file for future reference\n",
    "graph_filename = CSV_source_file                                                    #start with name of source file\n",
    "if CSV_source_file.lower().endswith(\".csv\"): graph_filename = CSV_source_file[:-4]  #remove the last 4 characters (.csv)\n",
    "graph_filename += \"_PieChart\"                                                       #append the type of chart (bar, pie, etc) to the filename\n",
    "graph_filename += \"_FileCount\"                                                      #append FileCount|ByteCount to the filename    \n",
    "graph_filename += \"_LastModificationDate\"                                           #append LastModificationDate or LastAccessDate to the filename\n",
    "graph_filename += \".png\"                                                            #append .png extension to the filename\n",
    "plt.savefig(graph_filename, format=\"png\", dpi=300, bbox_inches=\"tight\")\n",
    "print(f\"Graph saved as {graph_filename}\")\n",
    "\n",
    "# Show the chart\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf639c8-db67-40f7-b4c3-cb8da3d1df23",
   "metadata": {
    "id": "717e4bec"
   },
   "source": [
    "## 3.2 - Create graphs for byte counts\n",
    "(this section is time consuming if you have a lot of rows in the dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fIcuvI4smyuv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fIcuvI4smyuv",
    "outputId": "4c96312f-fb4a-4501-d8d9-86a4dd02f183"
   },
   "outputs": [],
   "source": [
    "## figure out the total number of bytes for each file category\n",
    "\n",
    "# Initialize variables as int64 to prevent overflow\n",
    "LastModified_bytes0to90days    = np.int64(0)\n",
    "LastModified_bytes90to180days  = np.int64(0)\n",
    "LastModified_bytes180to365days = np.int64(0)\n",
    "LastModified_bytes1to2years    = np.int64(0)\n",
    "LastModified_bytes2to3years    = np.int64(0)\n",
    "LastModified_bytes3to5years    = np.int64(0)\n",
    "LastModified_bytes5to99years   = np.int64(0)\n",
    "\n",
    "# If df.Bytes is int32 or uint32, it may overflow when adding large values. Convert it to int64:\n",
    "df[\"Bytes\"] = df[\"Bytes\"].astype(\"int64\")\n",
    "\n",
    "print(\"Starting processing of\", len(df), \"files\")\n",
    "if len(df) > 1000000: print(\"Detected large input file, please be patient during processing...\")\n",
    "# loop through the contents of the dataframe to find the total number of bytes for each file age category\n",
    "# Instead of using range(0, len(df)), iterate over the DataFrame's index:\n",
    "for i in df.index:\n",
    "    if (i % 100000 == 0):  print(\"Processed\", i, \"of\", len(df), \"files \", df.Filename[i], df.Bytes[i], df.ModificationTimeDays[i]) # print debug output every 100000 lines\n",
    "    # Check if i is the last index instead of relying on range:\n",
    "    if i == df.index[-1]: print(\"Processed\", i, \"of\", len(df), \"files \", df.Filename[i], df.Bytes[i], df.ModificationTimeDays[i]) # print debug output for the last line so we know when the loop is finished\n",
    "    if (df.ModificationTimeDays[i] >=0       and df.ModificationTimeDays[i] <  90):      LastModified_bytes0to90days    += df.Bytes[i]\n",
    "    if (df.ModificationTimeDays[i] >=90      and df.ModificationTimeDays[i] < 180):      LastModified_bytes90to180days  += df.Bytes[i]\n",
    "    if (df.ModificationTimeDays[i] >=180     and df.ModificationTimeDays[i] < 365):      LastModified_bytes180to365days += df.Bytes[i]\n",
    "    if (df.ModificationTimeDays[i] >=(365*1) and df.ModificationTimeDays[i] < (365*2)):  LastModified_bytes1to2years    += df.Bytes[i]\n",
    "    if (df.ModificationTimeDays[i] >=(365*2) and df.ModificationTimeDays[i] < (365*3)):  LastModified_bytes2to3years    += df.Bytes[i]\n",
    "    if (df.ModificationTimeDays[i] >=(365*3) and df.ModificationTimeDays[i] < (365*5)):  LastModified_bytes3to5years    += df.Bytes[i]\n",
    "    if (df.ModificationTimeDays[i] >=(365*5) and df.ModificationTimeDays[i] < (365*99)): LastModified_bytes5to99years   += df.Bytes[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abdd10d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3abdd10d",
    "outputId": "51f557bf-7b0a-4a18-c746-57a3b5e51d5f"
   },
   "outputs": [],
   "source": [
    "print(\"Bytes with last modification date   0 to  90  days: \", LastModified_bytes0to90days    )\n",
    "print(\"Bytes with last modification date  90 to 180  days: \", LastModified_bytes90to180days  )\n",
    "print(\"Bytes with last modification date 180 to 365  days: \", LastModified_bytes180to365days )\n",
    "print(\"Bytes with last modification date   1 to   2 years: \", LastModified_bytes1to2years    )\n",
    "print(\"Bytes with last modification date   2 to   3 years: \", LastModified_bytes2to3years    )\n",
    "print(\"Bytes with last modification date   3 to   5 years: \", LastModified_bytes3to5years    )\n",
    "print(\"Bytes with last modification date   5 to  99 years: \", LastModified_bytes5to99years   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4064d5-883e-49ea-a673-de3d9eebb5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm the datatype is int64 (avoids overflow for very large numbers)\n",
    "print(f\"Data type for df.Bytes is:\", df.Bytes.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802c0388",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "802c0388",
    "outputId": "3552fa59-26b3-41d3-f154-045075b28c53"
   },
   "outputs": [],
   "source": [
    "# convert bytes to more human-readable kilobytes for graphing\n",
    "# use int() to convert the floating point value to the nearest integer (nearest GB is close enough)\n",
    "LastModified_Kbytes0to90days    = int(LastModified_bytes0to90days/1024)\n",
    "LastModified_Kbytes90to180days  = int(LastModified_bytes90to180days/1024)\n",
    "LastModified_Kbytes180to365days = int(LastModified_bytes180to365days/1024)\n",
    "LastModified_Kbytes1to2years    = int(LastModified_bytes1to2years/1024)\n",
    "LastModified_Kbytes2to3years    = int(LastModified_bytes2to3years/1024)\n",
    "LastModified_Kbytes3to5years    = int(LastModified_bytes3to5years/1024)\n",
    "LastModified_Kbytes5to99years   = int(LastModified_bytes5to99years/1024)\n",
    "\n",
    "print(\"KiloBytes with last modification date   0 to  90  days: \", LastModified_Kbytes0to90days    )\n",
    "print(\"KiloBytes with last modification date  90 to 180  days: \", LastModified_Kbytes90to180days  )\n",
    "print(\"KiloBytes with last modification date 180 to 365  days: \", LastModified_Kbytes180to365days )\n",
    "print(\"KiloBytes with last modification date   1 to   2 years: \", LastModified_Kbytes1to2years    )\n",
    "print(\"KiloBytes with last modification date   2 to   3 years: \", LastModified_Kbytes2to3years    )\n",
    "print(\"KiloBytes with last modification date   3 to   5 years: \", LastModified_Kbytes3to5years    )\n",
    "print(\"KiloBytes with last modification date   5 to  99 years: \", LastModified_Kbytes5to99years   )\n",
    "print(\"-----------------------------------------------------------------\") \n",
    "\n",
    "\n",
    "\n",
    "# convert bytes to more human-readable megabytes for graphing\n",
    "# use int() to convert the floating point value to the nearest integer (nearest GB is close enough)\n",
    "LastModified_Mbytes0to90days    = int(LastModified_bytes0to90days/1024/1024)\n",
    "LastModified_Mbytes90to180days  = int(LastModified_bytes90to180days/1024/1024)\n",
    "LastModified_Mbytes180to365days = int(LastModified_bytes180to365days/1024/1024)\n",
    "LastModified_Mbytes1to2years    = int(LastModified_bytes1to2years/1024/1024)\n",
    "LastModified_Mbytes2to3years    = int(LastModified_bytes2to3years/1024/1024)\n",
    "LastModified_Mbytes3to5years    = int(LastModified_bytes3to5years/1024/1024)\n",
    "LastModified_Mbytes5to99years   = int(LastModified_bytes5to99years/1024/1024)\n",
    "\n",
    "print(\"MegaBytes with last modification date   0 to  90  days: \", LastModified_Mbytes0to90days    )\n",
    "print(\"MegaBytes with last modification date  90 to 180  days: \", LastModified_Mbytes90to180days  )\n",
    "print(\"MegaBytes with last modification date 180 to 365  days: \", LastModified_Mbytes180to365days )\n",
    "print(\"MegaBytes with last modification date   1 to   2 years: \", LastModified_Mbytes1to2years    )\n",
    "print(\"MegaBytes with last modification date   2 to   3 years: \", LastModified_Mbytes2to3years    )\n",
    "print(\"MegaBytes with last modification date   3 to   5 years: \", LastModified_Mbytes3to5years    )\n",
    "print(\"MegaBytes with last modification date   5 to  99 years: \", LastModified_Mbytes5to99years   )\n",
    "print(\"-----------------------------------------------------------------\") \n",
    "\n",
    "\n",
    "\n",
    "# convert bytes to more human-readable gigabytes for graphing\n",
    "# use int() to convert the floating point value to the nearest integer (nearest MB is close enough)\n",
    "LastModified_Gbytes0to90days    = int(LastModified_bytes0to90days/1024/1024/1024)\n",
    "LastModified_Gbytes90to180days  = int(LastModified_bytes90to180days/1024/1024/1024)\n",
    "LastModified_Gbytes180to365days = int(LastModified_bytes180to365days/1024/1024/1024)\n",
    "LastModified_Gbytes1to2years    = int(LastModified_bytes1to2years/1024/1024/1024)\n",
    "LastModified_Gbytes2to3years    = int(LastModified_bytes2to3years/1024/1024/1024)\n",
    "LastModified_Gbytes3to5years    = int(LastModified_bytes3to5years/1024/1024/1024)\n",
    "LastModified_Gbytes5to99years   = int(LastModified_bytes5to99years/1024/1024/1024)\n",
    "\n",
    "print(\"GigaBytes with last modification date   0 to  90  days: \", LastModified_Gbytes0to90days    )\n",
    "print(\"GigaBytes with last modification date  90 to 180  days: \", LastModified_Gbytes90to180days  )\n",
    "print(\"GigaBytes with last modification date 180 to 365  days: \", LastModified_Gbytes180to365days )\n",
    "print(\"GigaBytes with last modification date   1 to   2 years: \", LastModified_Gbytes1to2years    )\n",
    "print(\"GigaBytes with last modification date   2 to   3 years: \", LastModified_Gbytes2to3years    )\n",
    "print(\"GigaBytes with last modification date   3 to   5 years: \", LastModified_Gbytes3to5years    )\n",
    "print(\"GigaBytes with last modification date   5 to  99 years: \", LastModified_Gbytes5to99years   )\n",
    "print(\"-----------------------------------------------------------------\") \n",
    "\n",
    "\n",
    "# convert bytes to more human-readable terabytes for graphing\n",
    "# use int() to convert the floating point value to the nearest integer (nearest TB is close enough)\n",
    "LastModified_Tbytes0to90days    = int(LastModified_bytes0to90days/1024/1024/1024/1024)\n",
    "LastModified_Tbytes90to180days  = int(LastModified_bytes90to180days/1024/1024/1024/1024)\n",
    "LastModified_Tbytes180to365days = int(LastModified_bytes180to365days/1024/1024/1024/1024)\n",
    "LastModified_Tbytes1to2years    = int(LastModified_bytes1to2years/1024/1024/1024/1024)\n",
    "LastModified_Tbytes2to3years    = int(LastModified_bytes2to3years/1024/1024/1024/1024)\n",
    "LastModified_Tbytes3to5years    = int(LastModified_bytes3to5years/1024/1024/1024/1024)\n",
    "LastModified_Tbytes5to99years   = int(LastModified_bytes5to99years/1024/1024/1024/1024)\n",
    "\n",
    "print(\"TeraBytes with last modification date   0 to  90  days: \", LastModified_Tbytes0to90days    )\n",
    "print(\"TeraBytes with last modification date  90 to 180  days: \", LastModified_Tbytes90to180days  )\n",
    "print(\"TeraBytes with last modification date 180 to 365  days: \", LastModified_Tbytes180to365days )\n",
    "print(\"TeraBytes with last modification date   1 to   2 years: \", LastModified_Tbytes1to2years    )\n",
    "print(\"TeraBytes with last modification date   2 to   3 years: \", LastModified_Tbytes2to3years    )\n",
    "print(\"TeraBytes with last modification date   3 to   5 years: \", LastModified_Tbytes3to5years    )\n",
    "print(\"TeraBytes with last modification date   5 to  99 years: \", LastModified_Tbytes5to99years   )\n",
    "print(\"-----------------------------------------------------------------\") \n",
    "\n",
    "\n",
    "# convert bytes to more human-readable petabytes for graphing\n",
    "# use int() to convert the floating point value to the nearest integer (nearest PB is close enough)\n",
    "LastModified_Pbytes0to90days    = int(LastModified_bytes0to90days/1024/1024/1024/1024/1024)\n",
    "LastModified_Pbytes90to180days  = int(LastModified_bytes90to180days/1024/1024/1024/1024/1024)\n",
    "LastModified_Pbytes180to365days = int(LastModified_bytes180to365days/1024/1024/1024/1024/1024)\n",
    "LastModified_Pbytes1to2years    = int(LastModified_bytes1to2years/1024/1024/1024/1024/1024)\n",
    "LastModified_Pbytes2to3years    = int(LastModified_bytes2to3years/1024/1024/1024/1024/1024)\n",
    "LastModified_Pbytes3to5years    = int(LastModified_bytes3to5years/1024/1024/1024/1024/1024)\n",
    "LastModified_Pbytes5to99years   = int(LastModified_bytes5to99years/1024/1024/1024/1024/1024)\n",
    "\n",
    "print(\"PetaBytes with last modification date   0 to  90  days: \", LastModified_Pbytes0to90days    )\n",
    "print(\"PetaBytes with last modification date  90 to 180  days: \", LastModified_Pbytes90to180days  )\n",
    "print(\"PetaBytes with last modification date 180 to 365  days: \", LastModified_Pbytes180to365days )\n",
    "print(\"PetaBytes with last modification date   1 to   2 years: \", LastModified_Pbytes1to2years    )\n",
    "print(\"PetaBytes with last modification date   2 to   3 years: \", LastModified_Pbytes2to3years    )\n",
    "print(\"PetaBytes with last modification date   3 to   5 years: \", LastModified_Pbytes3to5years    )\n",
    "print(\"PetaBytes with last modification date   5 to  99 years: \", LastModified_Pbytes5to99years   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3698f7a0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3698f7a0",
    "outputId": "030be4f1-d5e8-4246-fd65-d21e6f80b410"
   },
   "outputs": [],
   "source": [
    "# Based on the byte counts, figure out the preferred unit value (Bytes, MegaBytes, GigaBytes, TeraBytes) to use for graphing\n",
    "if (LastModified_bytes0to90days  >= 0):  y_axis_units = \"Bytes\"  #start with default value of bytes\n",
    "if (LastModified_Kbytes0to90days >= 10): y_axis_units = \"KiloBytes\"\n",
    "if (LastModified_Mbytes0to90days >= 10): y_axis_units = \"MegaBytes\"\n",
    "if (LastModified_Gbytes0to90days >= 10): y_axis_units = \"GigaBytes\"\n",
    "if (LastModified_Tbytes0to90days >= 10): y_axis_units = \"TeraBytes\"\n",
    "if (LastModified_Pbytes0to90days >= 10): y_axis_units = \"PetaBytes\"\n",
    "print (\"Based on file sizes, the vertical y-axis units will be shown in \", y_axis_units)\n",
    "\n",
    "## manually edit y_axis_units if desired\n",
    "##y_axis_units = \"GigaBytes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e04aa1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "d6e04aa1",
    "outputId": "60ab95fa-4751-4b28-dd0c-be4309fa0008",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a bar graph with these vertical columns:\n",
    "# bytes0to90days\n",
    "# bytes90to180days\n",
    "# bytes180to365days\n",
    "# bytes1to2years\n",
    "# bytes2to3years\n",
    "# bytes3to5years\n",
    "# bytes5to99years\n",
    "\n",
    "\n",
    "\n",
    "# function to add value labels\n",
    "def addlabels(x,y):\n",
    "    for i in range(len(x)):\n",
    "        plt.text(i, y[i], y[i], ha = 'center')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # creating data on which bar chart will be plot\n",
    "    x = \"0to90days\", \"90to180days\", \"180to365days\", \"1to2years\", \"2to3years\", \"3to5years\", \"5to99years\"\n",
    "    #\n",
    "    # Based on how big the numbers are, figure out if the vertical y-axis should be in bytes, Mbytes, Gbytes, Tbytes, Pbytes\n",
    "    if (y_axis_units == \"Bytes\"):     y = [ LastModified_bytes0to90days,  LastModified_bytes90to180days,  LastModified_bytes180to365days,  LastModified_bytes1to2years,  LastModified_bytes2to3years,  LastModified_bytes3to5years,  LastModified_bytes5to99years]\n",
    "    if (y_axis_units == \"MegaBytes\"): y = [LastModified_Mbytes0to90days, LastModified_Mbytes90to180days, LastModified_Mbytes180to365days, LastModified_Mbytes1to2years, LastModified_Mbytes2to3years, LastModified_Mbytes3to5years, LastModified_Mbytes5to99years]\n",
    "    if (y_axis_units == \"GigaBytes\"): y = [LastModified_Gbytes0to90days, LastModified_Gbytes90to180days, LastModified_Gbytes180to365days, LastModified_Gbytes1to2years, LastModified_Gbytes2to3years, LastModified_Gbytes3to5years, LastModified_Gbytes5to99years]\n",
    "    if (y_axis_units == \"TeraBytes\"): y = [LastModified_Tbytes0to90days, LastModified_Tbytes90to180days, LastModified_Tbytes180to365days, LastModified_Tbytes1to2years, LastModified_Tbytes2to3years, LastModified_Tbytes3to5years, LastModified_Tbytes5to99years]\n",
    "    if (y_axis_units == \"PetaBytes\"): y = [LastModified_Pbytes0to90days, LastModified_Pbytes90to180days, LastModified_Pbytes180to365days, LastModified_Pbytes1to2years, LastModified_Pbytes2to3years, LastModified_Pbytes3to5years, LastModified_Pbytes5to99years]\n",
    "\n",
    "\n",
    "    # setting figure size by using figure() function\n",
    "    plt.figure(figsize = (10, 5))\n",
    "\n",
    "    # making the bar chart on the data\n",
    "    plt.bar(x, y)\n",
    "\n",
    "    # calling the function to add value labels\n",
    "    addlabels(x, y)\n",
    "\n",
    "    # giving title to the plot\n",
    "    plt.title(\"Byte Counts by Last Modification Date\")\n",
    "\n",
    "    # giving X and Y labels\n",
    "    plt.xlabel(\"Last Modification Date\")\n",
    "    plt.ylabel(\"Number of bytes\")\n",
    "    if (y_axis_units == \"Bytes\"):     plt.ylabel(\"Bytes\")\n",
    "    if (y_axis_units == \"KiloBytes\"): plt.ylabel(\"Kilobytes\")\n",
    "    if (y_axis_units == \"MegaBytes\"): plt.ylabel(\"MegaBbytes\")\n",
    "    if (y_axis_units == \"GigaBytes\"): plt.ylabel(\"GigaBytes\")\n",
    "    if (y_axis_units == \"TeraBytes\"): plt.ylabel(\"TeraBytes\")\n",
    "    if (y_axis_units == \"PetaBytes\"): plt.ylabel(\"PetaBytes\")\n",
    "\n",
    "    # Save the graph as a PNG file for future reference\n",
    "    graph_filename = CSV_source_file                                                    #start with name of source file\n",
    "    if CSV_source_file.lower().endswith(\".csv\"): graph_filename = CSV_source_file[:-4]  # Remove the last 4 characters (.csv)\n",
    "    graph_filename += \"_BarChart\"                                                       #append the type of chart (bar, pie, etc) to the filename\n",
    "    graph_filename += \"_ByteCount\"                                                      #append FileCount|ByteCount to the filename    \n",
    "    graph_filename += \"_LastModificationDate\"                                           #append LastModificationDate or LastAccessDate to the filename\n",
    "    graph_filename += \".png\"                                                            #append .png extension to the filename\n",
    "    plt.savefig(graph_filename, format=\"png\", dpi=300, bbox_inches=\"tight\")\n",
    "    print(f\"Graph saved as {graph_filename}\")\n",
    "\n",
    "\n",
    "    # visualizing the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d049e0c5-72cc-4851-bc61-6b5eb3c82885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pie chart showing percentages\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "# Data for the pie chart\n",
    "labels = [\"0to90days\", \"90to180days\", \"180to365days\", \"1to2years\", \"2to3years\", \"3to5years\", \"5to99years\"]\n",
    "sizes  = [LastModified_bytes0to90days, LastModified_bytes90to180days, LastModified_bytes180to365days, LastModified_bytes1to2years, LastModified_bytes2to3years, LastModified_bytes3to5years, LastModified_bytes5to99years]\n",
    "\n",
    "# The chart gets cluttered and hard to read if there are labels on very small pie slices\n",
    "# Only put labels on pie slices larger than 10%\n",
    "# Function to format labels with percentage only if >10%\n",
    "def label_format(label, pct):\n",
    "    return f\"{label} \\n{pct:.1f}%\" if pct > 5 else \"\"  # Show label + % if >10%\n",
    "\n",
    "# Compute final labels with percentages for slices >5%\n",
    "total_size = sum(sizes)\n",
    "final_labels = [label_format(label, (size / total_size) * 100) for label, size in zip(labels, sizes)]\n",
    "\n",
    "# Set up figure size\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Create pie chart\n",
    "plt.pie(\n",
    "    sizes,\n",
    "    labels=final_labels,  # Labels include percentage only if >10%\n",
    "    labeldistance=0.5,    # Moves labels inside slices\n",
    ")\n",
    "\n",
    "# Set title\n",
    "plt.title(\"Percentage Byte Counts by Last Modification Date\")\n",
    "\n",
    "# Generate legend labels with percentages\n",
    "legend_labels = [f\"{label} ({(size / total_size) * 100:.1f}%)\" for label, size in zip(labels, sizes)]\n",
    "\n",
    "# Add legend with percentage values\n",
    "plt.legend(legend_labels, loc=\"best\", bbox_to_anchor=(1, 0.5), title=\"\")\n",
    "\n",
    "# Save the graph as a PNG file for future reference\n",
    "graph_filename = CSV_source_file                                                    #start with name of source file\n",
    "if CSV_source_file.lower().endswith(\".csv\"): graph_filename = CSV_source_file[:-4]  #remove the last 4 characters (.csv)\n",
    "graph_filename += \"_PieChart\"                                                       #append the type of chart (bar, pie, etc) to the filename\n",
    "graph_filename += \"_ByteCount\"                                                      #append FileCount|ByteCount to the filename    \n",
    "graph_filename += \"_LastModificationDate\"                                           #append LastModificationDate or LastAccessDate to the filename\n",
    "graph_filename += \".png\"                                                            #append .png extension to the filename\n",
    "plt.savefig(graph_filename, format=\"png\", dpi=300, bbox_inches=\"tight\")\n",
    "print(f\"Graph saved as {graph_filename}\")\n",
    "\n",
    "# Show the chart\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e96926",
   "metadata": {
    "id": "32e96926"
   },
   "source": [
    "## 3.3 - Create CSV reports for each category of files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50e607a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c50e607a",
    "outputId": "988b453d-260d-40cb-cb92-76ef9387a022"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Generate reports in CSV format showing the list of filenames, bytes, ModificationTimeDays\n",
    "\n",
    "## open file for writing\n",
    "#CSV_temp_file = CSV_source_file   #start with name of source file\n",
    "#CSV_temp_file += \".tmp\"           #append .tmp to filename\n",
    "\n",
    "CSV_output_file = CSV_source_file                                                   #start with name of source file\n",
    "if CSV_source_file.lower().endswith(\".csv\"): CSV_output_file = CSV_source_file[:-4]  #remove the last 4 characters (.csv)\n",
    "CSV_output_file += \"_LastModified_0to90days.csv\"                                    #append date range to filename\n",
    "df_output_file  = df[ (df['ModificationTimeDays'] >= 0)        & (df['ModificationTimeDays'] < 90)]\n",
    "print (\"Creating CSV output file showing all filenames with ages at:\", CSV_output_file)\n",
    "df_output_file.to_csv(CSV_output_file)\n",
    "\n",
    "CSV_output_file = CSV_source_file                                                   #start with name of source file\n",
    "if CSV_source_file.lower().endswith(\".csv\"): CSV_output_file = CSV_source_file[:-4]  #remove the last 4 characters (.csv)\n",
    "CSV_output_file += \"_LastModified_90to180days.csv\"                                  #append date range to filename\n",
    "df_output_file  = df[ (df['ModificationTimeDays'] >= 90)     & (df['ModificationTimeDays'] < 180)]\n",
    "print (\"Creating CSV output file showing all filenames with ages at:\", CSV_output_file)\n",
    "df_output_file.to_csv(CSV_output_file)\n",
    "\n",
    "CSV_output_file = CSV_source_file                                                   #start with name of source file\n",
    "if CSV_source_file.lower().endswith(\".csv\"): CSV_output_file = CSV_source_file[:-4]  #remove the last 4 characters (.csv)\n",
    "CSV_output_file += \"_LastModified_180to365days.csv\"                                 #append date range to filename\n",
    "df_output_file = df[ (df['ModificationTimeDays'] >= 180)   & (df['ModificationTimeDays'] < 365)]\n",
    "print (\"Creating CSV output file showing all filenames with ages at:\", CSV_output_file)\n",
    "df_output_file.to_csv(CSV_output_file)\n",
    "\n",
    "CSV_output_file = CSV_source_file                                                   #start with name of source file\n",
    "if CSV_source_file.lower().endswith(\".csv\"): CSV_output_file = CSV_source_file[:-4]  #remove the last 4 characters (.csv)\n",
    "CSV_output_file += \"_LastModified_1to2years.csv\"                                    #append date range to filename\n",
    "df_output_file = df[ (df['ModificationTimeDays'] >= (365*1))  & (df['ModificationTimeDays'] < (365*2))]\n",
    "print (\"Creating CSV output file showing all filenames with ages at:\", CSV_output_file)\n",
    "df_output_file.to_csv(CSV_output_file)\n",
    "\n",
    "CSV_output_file = CSV_source_file                                                   #start with name of source file\n",
    "if CSV_source_file.lower().endswith(\".csv\"): CSV_output_file = CSV_source_file[:-4]  #remove the last 4 characters (.csv)\n",
    "CSV_output_file += \"_LastModified_2to3years.csv\"                                    #append date range to filename\n",
    "df_output_file = df[ (df['ModificationTimeDays'] >= (365*2))  & (df['ModificationTimeDays'] < (365*3))]\n",
    "print (\"Creating CSV output file showing all filenames with ages at:\", CSV_output_file)\n",
    "df_output_file.to_csv(CSV_output_file)\n",
    "\n",
    "CSV_output_file = CSV_source_file                                                   #start with name of source file\n",
    "if CSV_source_file.lower().endswith(\".csv\"): CSV_output_file = CSV_source_file[:-4]  #remove the last 4 characters (.csv)\n",
    "CSV_output_file += \"_LastModified_3to5years.csv\"                                    #append date range to filename\n",
    "df_output_file = df[ (df['ModificationTimeDays'] >= (365*3))  & (df['ModificationTimeDays'] < (365*5))]\n",
    "print (\"Creating CSV output file showing all filenames with ages at:\", CSV_output_file)\n",
    "df_output_file.to_csv(CSV_output_file)\n",
    "\n",
    "CSV_output_file = CSV_source_file                                                   #start with name of source file\n",
    "if CSV_source_file.lower().endswith(\".csv\"): CSV_output_file = CSV_source_file[:-4]  #remove the last 4 characters (.csv)\n",
    "CSV_output_file += \"_LastModified_5to99years.csv\"                                   #append date range to filename\n",
    "df_output_file= df[ (df['ModificationTimeDays'] >= (365*5)) & (df['ModificationTimeDays'] < (365*99))]\n",
    "print (\"Creating CSV output file showing all filenames with ages at:\", CSV_output_file)\n",
    "df_output_file.to_csv(CSV_output_file)\n",
    "\n",
    "print (\"Finished creating CSV output files\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1411f9-2cd8-4011-9b3d-bf66cfca386d",
   "metadata": {},
   "source": [
    "# 4 - Categorize files by Last Access Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66df3822-6c12-4685-bd32-edeef27f017f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a list of all the filenames with age 0 to  90 days old by extracting all rows with ModificationTimeDays <= 90\n",
    "LastAccessed_age0to90days = (df[(df['AccessTimeDays'] >= 0) & (df['AccessTimeDays'] < 90)])\n",
    "print(\"Number of files with last access date   0 to  90  days: \", (len(LastAccessed_age0to90days)) )\n",
    "\n",
    "# show a list of all the filenames with age 90 to 180 days\n",
    "LastAccessed_age90to180days = (df[(df['AccessTimeDays'] >= 90) & (df['AccessTimeDays'] < 180)])\n",
    "print(\"Number of files with last access date  90 to 180  days: \", (len(LastModified_age90to180days)) )\n",
    "\n",
    "# show a list of all the filenames with age 180 to 365 days\n",
    "LastAccessed_age180to365days = (df[(df['AccessTimeDays'] >= 180) & (df['AccessTimeDays'] < 365)])\n",
    "print(\"Number of files with last access date 180 to 365  days: \", (len(LastModified_age180to365days)) )\n",
    "\n",
    "# show a list of all the filenames with age 1 to 2 years days\n",
    "LastAccessed_age1to2years = (df[(df['AccessTimeDays'] >= (365*1)) & (df['AccessTimeDays'] < (365*2))])\n",
    "print(\"Number of files with last access date   1 to   2 years: \", (len(LastModified_age1to2years)) )\n",
    "\n",
    "# show a list of all the filenames with age 2 to 3 years days\n",
    "LastAccessed_age2to3years = (df[(df['AccessTimeDays'] >= (365*2)) & (df['AccessTimeDays'] < (365*3))])\n",
    "print(\"Number of files with last access date   2 to   3 years: \", (len(LastModified_age2to3years)) )\n",
    "\n",
    "# show a list of all the filenames with age 3 to 5 years days\n",
    "LastAccessed_age3to5years = (df[(df['AccessTimeDays'] >= (365*3)) & (df['AccessTimeDays'] < (365*5))])\n",
    "print(\"Number of files with last access date   3 to   5 years: \", (len(LastModified_age3to5years)) )\n",
    "\n",
    "# show a list of all the filenames with age 5 to 99 years days\n",
    "LastAccessed_age5to99years = (df[(df['AccessTimeDays'] >= (365*5)) & (df['AccessTimeDays'] < (365*99))])\n",
    "print(\"Number of files with last access date   5 to  99 years: \", (len(LastModified_age5to99years)) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa82b565-8b28-4a93-8c12-348eb9e4a2b9",
   "metadata": {},
   "source": [
    "## 4.1 - Create graphs for file counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5c4f1f-370c-4e3e-b75f-bcceeec2c24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a bar chart \n",
    "\n",
    "# function to add value labels\n",
    "def addlabels(x,y):\n",
    "    for i in range(len(x)):\n",
    "        plt.text(i, y[i], y[i], ha = 'center')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # creating data on which bar chart will be plot\n",
    "    x = \"0to90days\", \"90to180days\", \"180to365days\", \"1to2years\", \"2to3years\", \"3to5years\", \"5to99years\"\n",
    "    y = [len(LastAccessed_age0to90days), len(LastAccessed_age90to180days), len(LastAccessed_age180to365days), len(LastAccessed_age1to2years), len(LastAccessed_age2to3years), len(LastAccessed_age3to5years), len(LastAccessed_age5to99years)]\n",
    "\n",
    "    # setting figure size by using figure() function\n",
    "    plt.figure(figsize = (10, 5))\n",
    "\n",
    "    # making the bar chart on the data\n",
    "    plt.bar(x, y)\n",
    "\n",
    "    # calling the function to add value labels\n",
    "    addlabels(x, y)\n",
    "\n",
    "    # Add main title (suptitle) and subtitle (title)\n",
    "    plt.suptitle(\"Filename counts by Last Access Date\", fontsize=14, fontweight=\"bold\", y=0.95)\n",
    "    plt.title(\"(not all filesystems record Last Access Date, may be less reliable than Last Modification Date)\", fontsize=8, pad=2)  # Subtitle with small padding\n",
    "\n",
    "    # giving X and Y labels\n",
    "    plt.xlabel(\"Last Access Date\")\n",
    "    plt.ylabel(\"Number of files\")\n",
    "\n",
    "    # Save the graph as a PNG file for future reference\n",
    "    graph_filename = CSV_source_file                                                    #start with name of source file\n",
    "    if CSV_source_file.lower().endswith(\".csv\"): graph_filename = CSV_source_file[:-4]  # Remove the last 4 characters (.csv)\n",
    "    graph_filename += \"_BarChart\"                                                       #append the type of chart (bar, pie, etc) to the filename\n",
    "    graph_filename += \"_FileCount\"                                                      #append FileCount|ByteCount to the filename    \n",
    "    graph_filename += \"_LastAccessDate\"                                                 #append LastModificationDate or LastAccessDate to the filename\n",
    "    graph_filename += \".png\"                                                            #append .png extension to the filename\n",
    "    plt.savefig(graph_filename, format=\"png\", dpi=300, bbox_inches=\"tight\")\n",
    "    print(f\"Graph saved as {graph_filename}\")\n",
    "\n",
    "\n",
    "    # visualizing the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1857b1a6-512b-4d56-aa41-fc05585cd1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data for the pie chart\n",
    "labels = [\"0to90days\", \"90to180days\", \"180to365days\", \"1to2years\", \"2to3years\", \"3to5years\", \"5to99years\"]\n",
    "sizes = [len(LastAccessed_age0to90days), len(LastAccessed_age90to180days), len(LastAccessed_age180to365days), len(LastAccessed_age1to2years), len(LastAccessed_age2to3years), len(LastAccessed_age3to5years), len(LastAccessed_age5to99years)]\n",
    "\n",
    "# The chart gets cluttered and hard to read if there are labels on very small pie slices\n",
    "# Only put labels on pie slices larger than 10%\n",
    "# Function to format labels with percentage only if >5%\n",
    "def label_format(label, pct):\n",
    "    return f\"{label} \\n{pct:.1f}%\" if pct > 5 else \"\"  # Show label + % if >10%\n",
    "\n",
    "# Compute final labels with percentages for slices >5%\n",
    "total_size = sum(sizes)\n",
    "final_labels = [label_format(label, (size / total_size) * 100) for label, size in zip(labels, sizes)]\n",
    "\n",
    "# Set up figure size\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Create pie chart\n",
    "plt.pie(\n",
    "    sizes,\n",
    "    labels=final_labels,  # Labels include percentage only if >10%\n",
    "    labeldistance=0.5,    # Moves labels inside slices\n",
    ")\n",
    "\n",
    "# Add main title (suptitle) and subtitle (title)\n",
    "plt.suptitle(\"Percentage Filename counts by Last Access Date\", fontsize=14, fontweight=\"bold\", y=0.95)\n",
    "plt.title(\"(not all filesystems record Last Access Date, may be less reliable than Last Modification Date)\", fontsize=8, pad=2)  # Subtitle with small padding\n",
    "\n",
    "\n",
    "# Generate legend labels with percentages\n",
    "legend_labels = [f\"{label} ({(size / total_size) * 100:.1f}%)\" for label, size in zip(labels, sizes)]\n",
    "\n",
    "# Add legend with percentage values\n",
    "plt.legend(legend_labels, loc=\"best\", bbox_to_anchor=(1, 0.5), title=\"\")\n",
    "\n",
    "# Save the graph as a PNG file for future reference\n",
    "graph_filename = CSV_source_file                                                    #start with name of source file\n",
    "if CSV_source_file.lower().endswith(\".csv\"): graph_filename = CSV_source_file[:-4]  #remove the last 4 characters (.csv)\n",
    "graph_filename += \"_PieChart\"                                                       #append the type of chart (bar, pie, etc) to the filename\n",
    "graph_filename += \"_FileCount\"                                                      #append FileCount|ByteCount to the filename    \n",
    "graph_filename += \"_LastAccessDate\"                                                 #append LastModificationDate or LastAccessDate to the filename\n",
    "graph_filename += \".png\"                                                            #append .png extension to the filename\n",
    "plt.savefig(graph_filename, format=\"png\", dpi=300, bbox_inches=\"tight\")\n",
    "print(f\"Graph saved as {graph_filename}\")\n",
    "\n",
    "# Show the chart\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49844e39-6321-48d6-8570-4910648a8ede",
   "metadata": {},
   "source": [
    "## 4.2 - Create graphs for byte counts\n",
    "(this section is time consuming if you have a lot of rows in the dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4ee3ab-8de9-4b2a-9b4a-3ae4f03e10ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## figure out the total number of bytes for each file category\n",
    "\n",
    "# Initialize variables as int64 to prevent overflow\n",
    "LastAccessed_bytes0to90days    = np.int64(0)\n",
    "LastAccessed_bytes90to180days  = np.int64(0)\n",
    "LastAccessed_bytes180to365days = np.int64(0)\n",
    "LastAccessed_bytes1to2years    = np.int64(0)\n",
    "LastAccessed_bytes2to3years    = np.int64(0)\n",
    "LastAccessed_bytes3to5years    = np.int64(0)\n",
    "LastAccessed_bytes5to99years   = np.int64(0)\n",
    "\n",
    "# If df.Bytes is int32 or uint32, it may overflow when adding large values. Convert it to int64:\n",
    "df[\"Bytes\"] = df[\"Bytes\"].astype(\"int64\")\n",
    "\n",
    "print(\"Starting processing of\", len(df), \"files\")\n",
    "if len(df) > 1000000: print(\"Detected large input file, please be patient during processing...\")\n",
    "# loop through the contents of the dataframe to find the total number of bytes for each file age category\n",
    "# Instead of using range(0, len(df)), iterate over the DataFrame's index:\n",
    "for i in df.index:\n",
    "    if (i % 100000 == 0):  print(\"Processed\", i, \"of\", len(df), \"files \", df.Filename[i], df.Bytes[i], df.AccessTimeDays[i]) # print debug output every 100000 lines\n",
    "    # Check if i is the last index instead of relying on range:\n",
    "    if i == df.index[-1]: print(\"Processed\", i, \"of\", len(df), \"files \", df.Filename[i], df.Bytes[i], df.ModificationTimeDays[i]) # print debug output for the last line so we know when the loop is finished\n",
    "    if (df.AccessTimeDays[i] >=0       and df.AccessTimeDays[i] <  90):      LastAccessed_bytes0to90days    += df.Bytes[i]\n",
    "    if (df.AccessTimeDays[i] >=90      and df.AccessTimeDays[i] < 180):      LastAccessed_bytes90to180days  += df.Bytes[i]\n",
    "    if (df.AccessTimeDays[i] >=180     and df.AccessTimeDays[i] < 365):      LastAccessed_bytes180to365days += df.Bytes[i]\n",
    "    if (df.AccessTimeDays[i] >=(365*1) and df.AccessTimeDays[i] < (365*2)):  LastAccessed_bytes1to2years    += df.Bytes[i]\n",
    "    if (df.AccessTimeDays[i] >=(365*2) and df.AccessTimeDays[i] < (365*3)):  LastAccessed_bytes2to3years    += df.Bytes[i]\n",
    "    if (df.AccessTimeDays[i] >=(365*3) and df.AccessTimeDays[i] < (365*5)):  LastAccessed_bytes3to5years    += df.Bytes[i]\n",
    "    if (df.AccessTimeDays[i] >=(365*5) and df.AccessTimeDays[i] < (365*99)): LastAccessed_bytes5to99years   += df.Bytes[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc07127-48ff-489a-ade0-8de21f46db05",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Bytes with last access date   0 to  90  days: \", LastAccessed_bytes0to90days    )\n",
    "print(\"Bytes with last access date  90 to 180  days: \", LastAccessed_bytes90to180days  )\n",
    "print(\"Bytes with last access date 180 to 365  days: \", LastAccessed_bytes180to365days )\n",
    "print(\"Bytes with last access date   1 to   2 years: \", LastAccessed_bytes1to2years    )\n",
    "print(\"Bytes with last access date   2 to   3 years: \", LastAccessed_bytes2to3years    )\n",
    "print(\"Bytes with last access date   3 to   5 years: \", LastAccessed_bytes3to5years    )\n",
    "print(\"Bytes with last access date   5 to  99 years: \", LastAccessed_bytes5to99years   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674a4b49-78f2-4354-86b3-f29628da3d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm the datatype is int64 (avoids overflow for very large numbers)\n",
    "print(f\"Data type for df.Bytes is:\", df.Bytes.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9810ace9-7048-448f-a404-dc7d1cc43149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert bytes to more human-readable kilobytes for graphing\n",
    "# use int() to convert the floating point value to the nearest integer (nearest GB is close enough)\n",
    "LastAccessed_Kbytes0to90days    = int(LastAccessed_bytes0to90days/1024)\n",
    "LastAccessed_Kbytes90to180days  = int(LastAccessed_bytes90to180days/1024)\n",
    "LastAccessed_Kbytes180to365days = int(LastAccessed_bytes180to365days/1024)\n",
    "LastAccessed_Kbytes1to2years    = int(LastAccessed_bytes1to2years/1024)\n",
    "LastAccessed_Kbytes2to3years    = int(LastAccessed_bytes2to3years/1024)\n",
    "LastAccessed_Kbytes3to5years    = int(LastAccessed_bytes3to5years/1024)\n",
    "LastAccessed_Kbytes5to99years   = int(LastAccessed_bytes5to99years/1024)\n",
    "\n",
    "print(\"KiloBytes with last access date   0 to  90  days: \", LastAccessed_Kbytes0to90days    )\n",
    "print(\"KiloBytes with last access date  90 to 180  days: \", LastAccessed_Kbytes90to180days  )\n",
    "print(\"KiloBytes with last access date 180 to 365  days: \", LastAccessed_Kbytes180to365days )\n",
    "print(\"KiloBytes with last access date   1 to   2 years: \", LastAccessed_Kbytes1to2years    )\n",
    "print(\"KiloBytes with last access date   2 to   3 years: \", LastAccessed_Kbytes2to3years    )\n",
    "print(\"KiloBytes with last access date   3 to   5 years: \", LastAccessed_Kbytes3to5years    )\n",
    "print(\"KiloBytes with last access date   5 to  99 years: \", LastAccessed_Kbytes5to99years   )\n",
    "print(\"-----------------------------------------------------------------\") \n",
    "\n",
    "\n",
    "# convert bytes to more human-readable megabytes for graphing\n",
    "# use int() to convert the floating point value to the nearest integer (nearest GB is close enough)\n",
    "LastAccessed_Mbytes0to90days    = int(LastAccessed_bytes0to90days/1024/1024)\n",
    "LastAccessed_Mbytes90to180days  = int(LastAccessed_bytes90to180days/1024/1024)\n",
    "LastAccessed_Mbytes180to365days = int(LastAccessed_bytes180to365days/1024/1024)\n",
    "LastAccessed_Mbytes1to2years    = int(LastAccessed_bytes1to2years/1024/1024)\n",
    "LastAccessed_Mbytes2to3years    = int(LastAccessed_bytes2to3years/1024/1024)\n",
    "LastAccessed_Mbytes3to5years    = int(LastAccessed_bytes3to5years/1024/1024)\n",
    "LastAccessed_Mbytes5to99years   = int(LastAccessed_bytes5to99years/1024/1024)\n",
    "\n",
    "print(\"MegaBytes with last access date   0 to  90  days: \", LastAccessed_Mbytes0to90days    )\n",
    "print(\"MegaBytes with last access date  90 to 180  days: \", LastAccessed_Mbytes90to180days  )\n",
    "print(\"MegaBytes with last access date 180 to 365  days: \", LastAccessed_Mbytes180to365days )\n",
    "print(\"MegaBytes with last access date   1 to   2 years: \", LastAccessed_Mbytes1to2years    )\n",
    "print(\"MegaBytes with last access date   2 to   3 years: \", LastAccessed_Mbytes2to3years    )\n",
    "print(\"MegaBytes with last access date   3 to   5 years: \", LastAccessed_Mbytes3to5years    )\n",
    "print(\"MegaBytes with last access date   5 to  99 years: \", LastAccessed_Mbytes5to99years   )\n",
    "print(\"-----------------------------------------------------------------\") \n",
    "\n",
    "\n",
    "# convert bytes to more human-readable gigabytes for graphing\n",
    "# use int() to convert the floating point value to the nearest integer (nearest MB is close enough)\n",
    "LastAccessed_Gbytes0to90days    = int(LastAccessed_bytes0to90days/1024/1024/1024)\n",
    "LastAccessed_Gbytes90to180days  = int(LastAccessed_bytes90to180days/1024/1024/1024)\n",
    "LastAccessed_Gbytes180to365days = int(LastAccessed_bytes180to365days/1024/1024/1024)\n",
    "LastAccessed_Gbytes1to2years    = int(LastAccessed_bytes1to2years/1024/1024/1024)\n",
    "LastAccessed_Gbytes2to3years    = int(LastAccessed_bytes2to3years/1024/1024/1024)\n",
    "LastAccessed_Gbytes3to5years    = int(LastAccessed_bytes3to5years/1024/1024/1024)\n",
    "LastAccessed_Gbytes5to99years   = int(LastAccessed_bytes5to99years/1024/1024/1024)\n",
    "\n",
    "print(\"GigaBytes with last access date   0 to  90  days: \", LastAccessed_Gbytes0to90days    )\n",
    "print(\"GigaBytes with last access date  90 to 180  days: \", LastAccessed_Gbytes90to180days  )\n",
    "print(\"GigaBytes with last access date 180 to 365  days: \", LastAccessed_Gbytes180to365days )\n",
    "print(\"GigaBytes with last access date   1 to   2 years: \", LastAccessed_Gbytes1to2years    )\n",
    "print(\"GigaBytes with last access date   2 to   3 years: \", LastAccessed_Gbytes2to3years    )\n",
    "print(\"GigaBytes with last access date   3 to   5 years: \", LastAccessed_Gbytes3to5years    )\n",
    "print(\"GigaBytes with last access date   5 to  99 years: \", LastAccessed_Gbytes5to99years   )\n",
    "print(\"-----------------------------------------------------------------\") \n",
    "\n",
    "\n",
    "# convert bytes to more human-readable terabytes for graphing\n",
    "# use int() to convert the floating point value to the nearest integer (nearest TB is close enough)\n",
    "LastAccessed_Tbytes0to90days    = int(LastAccessed_bytes0to90days/1024/1024/1024/1024)\n",
    "LastAccessed_Tbytes90to180days  = int(LastAccessed_bytes90to180days/1024/1024/1024/1024)\n",
    "LastAccessed_Tbytes180to365days = int(LastAccessed_bytes180to365days/1024/1024/1024/1024)\n",
    "LastAccessed_Tbytes1to2years    = int(LastAccessed_bytes1to2years/1024/1024/1024/1024)\n",
    "LastAccessed_Tbytes2to3years    = int(LastAccessed_bytes2to3years/1024/1024/1024/1024)\n",
    "LastAccessed_Tbytes3to5years    = int(LastAccessed_bytes3to5years/1024/1024/1024/1024)\n",
    "LastAccessed_Tbytes5to99years   = int(LastAccessed_bytes5to99years/1024/1024/1024/1024)\n",
    "\n",
    "print(\"TeraBytes with last access date   0 to  90  days: \", LastAccessed_Tbytes0to90days    )\n",
    "print(\"TeraBytes with last access date  90 to 180  days: \", LastAccessed_Tbytes90to180days  )\n",
    "print(\"TeraBytes with last access date 180 to 365  days: \", LastAccessed_Tbytes180to365days )\n",
    "print(\"TeraBytes with last access date   1 to   2 years: \", LastAccessed_Tbytes1to2years    )\n",
    "print(\"TeraBytes with last access date   2 to   3 years: \", LastAccessed_Tbytes2to3years    )\n",
    "print(\"TeraBytes with last access date   3 to   5 years: \", LastAccessed_Tbytes3to5years    )\n",
    "print(\"TeraBytes with last access date   5 to  99 years: \", LastAccessed_Tbytes5to99years   )\n",
    "print(\"-----------------------------------------------------------------\") \n",
    "\n",
    "\n",
    "# convert bytes to more human-readable petabytes for graphing\n",
    "# use int() to convert the floating point value to the nearest integer (nearest PB is close enough)\n",
    "LastAccessed_Pbytes0to90days    = int(LastAccessed_bytes0to90days/1024/1024/1024/1024/1024)\n",
    "LastAccessed_Pbytes90to180days  = int(LastAccessed_bytes90to180days/1024/1024/1024/1024/1024)\n",
    "LastAccessed_Pbytes180to365days = int(LastAccessed_bytes180to365days/1024/1024/1024/1024/1024)\n",
    "LastAccessed_Pbytes1to2years    = int(LastAccessed_bytes1to2years/1024/1024/1024/1024/1024)\n",
    "LastAccessed_Pbytes2to3years    = int(LastAccessed_bytes2to3years/1024/1024/1024/1024/1024)\n",
    "LastAccessed_Pbytes3to5years    = int(LastAccessed_bytes3to5years/1024/1024/1024/1024/1024)\n",
    "LastAccessed_Pbytes5to99years   = int(LastAccessed_bytes5to99years/1024/1024/1024/1024/1024)\n",
    "\n",
    "print(\"PetaBytes with last access date   0 to  90  days: \", LastAccessed_Pbytes0to90days    )\n",
    "print(\"PetaBytes with last access date  90 to 180  days: \", LastAccessed_Pbytes90to180days  )\n",
    "print(\"PetaBytes with last access date 180 to 365  days: \", LastAccessed_Pbytes180to365days )\n",
    "print(\"PetaBytes with last access date   1 to   2 years: \", LastAccessed_Pbytes1to2years    )\n",
    "print(\"PetaBytes with last access date   2 to   3 years: \", LastAccessed_Pbytes2to3years    )\n",
    "print(\"PetaBytes with last access date   3 to   5 years: \", LastAccessed_Pbytes3to5years    )\n",
    "print(\"PetaBytes with last access date   5 to  99 years: \", LastAccessed_Pbytes5to99years   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028dd91d-bc89-46a6-9b47-d030e1393037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the byte counts, figure out the preferred unit value (Bytes, MegaBytes, GigaBytes, TeraBytes) to use for graphing\n",
    "if (LastAccessed_bytes0to90days  >= 0):  y_axis_units = \"Bytes\"  #start with default value of bytes\n",
    "if (LastAccessed_Kbytes0to90days >= 10): y_axis_units = \"KiloBytes\"\n",
    "if (LastAccessed_Mbytes0to90days >= 10): y_axis_units = \"MegaBytes\"\n",
    "if (LastAccessed_Gbytes0to90days >= 10): y_axis_units = \"GigaBytes\"\n",
    "if (LastAccessed_Tbytes0to90days >= 10): y_axis_units = \"TeraBytes\"\n",
    "if (LastAccessed_Pbytes0to90days >= 10): y_axis_units = \"PetaBytes\"\n",
    "print (\"Based on file sizes, the vertical y-axis units will be shown in \", y_axis_units)\n",
    "\n",
    "## manually edit y_axis_units if desired\n",
    "##y_axis_units = \"GigaBytes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41f8a49-849a-419f-9ac1-c64346d363b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a bar graph with these vertical columns:\n",
    "# bytes0to90days\n",
    "# bytes90to180days\n",
    "# bytes180to365days\n",
    "# bytes1to2years\n",
    "# bytes2to3years\n",
    "# bytes3to5years\n",
    "# bytes5to99years\n",
    "\n",
    "\n",
    "\n",
    "# function to add value labels\n",
    "def addlabels(x,y):\n",
    "    for i in range(len(x)):\n",
    "        plt.text(i, y[i], y[i], ha = 'center')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # creating data on which bar chart will be plot\n",
    "    x = \"0to90days\", \"90to180days\", \"180to365days\", \"1to2years\", \"2to3years\", \"3to5years\", \"5to99years\"\n",
    "    #\n",
    "    # Based on how big the numbers are, figure out if the vertical y-axis should be in bytes, Mbytes, Gbytes, Tbytes, Pbytes\n",
    "    if (y_axis_units == \"Bytes\"):     y = [ LastAccessed_bytes0to90days,  LastAccessed_bytes90to180days,  LastAccessed_bytes180to365days,  LastAccessed_bytes1to2years,  LastAccessed_bytes2to3years,  LastAccessed_bytes3to5years,  LastAccessed_bytes5to99years]\n",
    "    if (y_axis_units == \"MegaBytes\"): y = [LastAccessed_Mbytes0to90days, LastAccessed_Mbytes90to180days, LastAccessed_Mbytes180to365days, LastAccessed_Mbytes1to2years, LastAccessed_Mbytes2to3years, LastAccessed_Mbytes3to5years, LastAccessed_Mbytes5to99years]\n",
    "    if (y_axis_units == \"GigaBytes\"): y = [LastAccessed_Gbytes0to90days, LastAccessed_Gbytes90to180days, LastAccessed_Gbytes180to365days, LastAccessed_Gbytes1to2years, LastAccessed_Gbytes2to3years, LastAccessed_Gbytes3to5years, LastAccessed_Gbytes5to99years]\n",
    "    if (y_axis_units == \"TeraBytes\"): y = [LastAccessed_Tbytes0to90days, LastAccessed_Tbytes90to180days, LastAccessed_Tbytes180to365days, LastAccessed_Tbytes1to2years, LastAccessed_Tbytes2to3years, LastAccessed_Tbytes3to5years, LastAccessed_Tbytes5to99years]\n",
    "    if (y_axis_units == \"PetaBytes\"): y = [LastAccessed_Pbytes0to90days, LastAccessed_Pbytes90to180days, LastAccessed_Pbytes180to365days, LastAccessed_Pbytes1to2years, LastAccessed_Pbytes2to3years, LastAccessed_Pbytes3to5years, LastAccessed_Pbytes5to99years]\n",
    "\n",
    "\n",
    "    # setting figure size by using figure() function\n",
    "    plt.figure(figsize = (10, 5))\n",
    "\n",
    "    # making the bar chart on the data\n",
    "    plt.bar(x, y)\n",
    "\n",
    "    # calling the function to add value labels\n",
    "    addlabels(x, y)\n",
    "\n",
    "    # Add main title (suptitle) and subtitle (title)\n",
    "    plt.suptitle(\"Byte Counts by Last Access Date\", fontsize=14, fontweight=\"bold\", y=0.95)\n",
    "    plt.title(\"(not all filesystems record Last Access Date, may be less reliable than Last Modification Date)\", fontsize=8, pad=2)  # Subtitle with small padding\n",
    "\n",
    "    # giving X and Y labels\n",
    "    plt.xlabel(\"Last Modification Date\")\n",
    "    plt.ylabel(\"Number of bytes\")\n",
    "    if (y_axis_units == \"Bytes\"):     plt.ylabel(\"Bytes\")\n",
    "    if (y_axis_units == \"KiloBytes\"): plt.ylabel(\"Kilobytes\")\n",
    "    if (y_axis_units == \"MegaBytes\"): plt.ylabel(\"MegaBbytes\")\n",
    "    if (y_axis_units == \"GigaBytes\"): plt.ylabel(\"GigaBytes\")\n",
    "    if (y_axis_units == \"TeraBytes\"): plt.ylabel(\"TeraBytes\")\n",
    "    if (y_axis_units == \"PetaBytes\"): plt.ylabel(\"PetaBytes\")\n",
    "\n",
    "    # Save the graph as a PNG file for future reference\n",
    "    graph_filename = CSV_source_file                                                    #start with name of source file\n",
    "    if CSV_source_file.lower().endswith(\".csv\"): graph_filename = CSV_source_file[:-4]  # Remove the last 4 characters (.csv)\n",
    "    graph_filename += \"_BarChart\"                                                       #append the type of chart (bar, pie, etc) to the filename\n",
    "    graph_filename += \"_ByteCount\"                                                      #append FileCount|ByteCount to the filename    \n",
    "    graph_filename += \"_LastAccessDate\"                                                 #append LastModificationDate or LastAccessDate to the filename\n",
    "    graph_filename += \".png\"                                                            #append .png extension to the filename\n",
    "    plt.savefig(graph_filename, format=\"png\", dpi=300, bbox_inches=\"tight\")\n",
    "    print(f\"Graph saved as {graph_filename}\")\n",
    "\n",
    "    # visualizing the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755e78f9-eac4-4f2b-924e-6e2f7115fc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data for the pie chart\n",
    "labels = [\"0to90days\", \"90to180days\", \"180to365days\", \"1to2years\", \"2to3years\", \"3to5years\", \"5to99years\"]\n",
    "sizes  = [LastAccessed_bytes0to90days, LastAccessed_bytes90to180days, LastAccessed_bytes180to365days, LastAccessed_bytes1to2years, LastAccessed_bytes2to3years, LastAccessed_bytes3to5years, LastAccessed_bytes5to99years]\n",
    "\n",
    "# The chart gets cluttered and hard to read if there are labels on very small pie slices\n",
    "# Only put labels on pie slices larger than 10%\n",
    "# Function to format labels with percentage only if >10%\n",
    "def label_format(label, pct):\n",
    "    return f\"{label} \\n{pct:.1f}%\" if pct > 5 else \"\"  # Show label + % if >10%\n",
    "\n",
    "# Compute final labels with percentages for slices >5%\n",
    "total_size = sum(sizes)\n",
    "final_labels = [label_format(label, (size / total_size) * 100) for label, size in zip(labels, sizes)]\n",
    "\n",
    "# Set up figure size\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Create pie chart\n",
    "plt.pie(\n",
    "    sizes,\n",
    "    labels=final_labels,  # Labels include percentage only if >10%\n",
    "    labeldistance=0.5,    # Moves labels inside slices\n",
    ")\n",
    "\n",
    "# Add main title (suptitle) and subtitle (title)\n",
    "plt.suptitle(\"Percentage Byte Counts by Last Access Date\", fontsize=14, fontweight=\"bold\", y=0.95)\n",
    "plt.title(\"(not all filesystems record Last Access Date, may be less reliable than Last Modification Date)\", fontsize=8, pad=2)  # Subtitle with small padding\n",
    "\n",
    "# Generate legend labels with percentages\n",
    "legend_labels = [f\"{label} ({(size / total_size) * 100:.1f}%)\" for label, size in zip(labels, sizes)]\n",
    "\n",
    "# Add legend with percentage values\n",
    "plt.legend(legend_labels, loc=\"best\", bbox_to_anchor=(1, 0.5), title=\"\")\n",
    "\n",
    "# Save the graph as a PNG file for future reference\n",
    "graph_filename = CSV_source_file                                                    #start with name of source file\n",
    "if CSV_source_file.lower().endswith(\".csv\"): graph_filename = CSV_source_file[:-4]  #remove the last 4 characters (.csv)\n",
    "graph_filename += \"_PieChart\"                                                       #append the type of chart (bar, pie, etc) to the filename\n",
    "graph_filename += \"_ByteCount\"                                                      #append FileCount|ByteCount to the filename    \n",
    "graph_filename += \"_LastAccessDate\"                                                 #append LastModificationDate or LastAccessDate to the filename\n",
    "graph_filename += \".png\"                                                            #append .png extension to the filename\n",
    "plt.savefig(graph_filename, format=\"png\", dpi=300, bbox_inches=\"tight\")\n",
    "print(f\"Graph saved as {graph_filename}\")\n",
    "\n",
    "# Show the chart\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55256606-dd13-4907-a81e-9bb947cf4457",
   "metadata": {},
   "source": [
    "## 4.3 - Create CSV reports for each category of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc056e12-fe48-4ef0-8617-c83d5bb69d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate reports in CSV format showing the list of filenames, bytes, ModificationTimeDays\n",
    "\n",
    "## open file for writing\n",
    "#CSV_temp_file = CSV_source_file   #start with name of source file\n",
    "#CSV_temp_file += \".tmp\"           #append .tmp to filename\n",
    "\n",
    "CSV_output_file = CSV_source_file                                                   #start with name of source file\n",
    "if CSV_source_file.lower().endswith(\".csv\"): CSV_output_file = CSV_source_file[:-4]  #remove the last 4 characters (.csv)\n",
    "CSV_output_file += \"_LastAccessed_0to90days.csv\"                                    #append date range to filename\n",
    "df_output_file  = df[ (df['AccessTimeDays'] >= 0)        & (df['AccessTimeDays'] < 90)]\n",
    "print (\"Creating CSV output file showing all filenames with ages at:\", CSV_output_file)\n",
    "df_output_file.to_csv(CSV_output_file)\n",
    "\n",
    "CSV_output_file = CSV_source_file                                                   #start with name of source file\n",
    "if CSV_source_file.lower().endswith(\".csv\"): CSV_output_file = CSV_source_file[:-4]  #remove the last 4 characters (.csv)\n",
    "CSV_output_file += \"_LastAccessed_90to180days.csv\"                                  #append date range to filename\n",
    "df_output_file  = df[ (df['AccessTimeDays'] >= 90)     & (df['AccessTimeDays'] < 180)]\n",
    "print (\"Creating CSV output file showing all filenames with ages at:\", CSV_output_file)\n",
    "df_output_file.to_csv(CSV_output_file)\n",
    "\n",
    "CSV_output_file = CSV_source_file                                                   #start with name of source file\n",
    "if CSV_source_file.lower().endswith(\".csv\"): CSV_output_file = CSV_source_file[:-4]  #remove the last 4 characters (.csv)\n",
    "CSV_output_file += \"_LastAccessed_180to365days.csv\"                                 #append date range to filename\n",
    "df_output_file = df[ (df['AccessTimeDays'] >= 180)   & (df['AccessTimeDays'] < 365)]\n",
    "print (\"Creating CSV output file showing all filenames with ages at:\", CSV_output_file)\n",
    "df_output_file.to_csv(CSV_output_file)\n",
    "\n",
    "CSV_output_file = CSV_source_file                                                   #start with name of source file\n",
    "if CSV_source_file.lower().endswith(\".csv\"): CSV_output_file = CSV_source_file[:-4]  #remove the last 4 characters (.csv)\n",
    "CSV_output_file += \"_LastAccessed_1to2years.csv\"                                    #append date range to filename\n",
    "df_output_file = df[ (df['AccessTimeDays'] >= (365*1))  & (df['AccessTimeDays'] < (365*2))]\n",
    "print (\"Creating CSV output file showing all filenames with ages at:\", CSV_output_file)\n",
    "df_output_file.to_csv(CSV_output_file)\n",
    "\n",
    "CSV_output_file = CSV_source_file                                                   #start with name of source file\n",
    "if CSV_source_file.lower().endswith(\".csv\"): CSV_output_file = CSV_source_file[:-4]  #remove the last 4 characters (.csv)\n",
    "CSV_output_file += \"_LastAccessed_2to3years.csv\"                                    #append date range to filename\n",
    "df_output_file = df[ (df['AccessTimeDays'] >= (365*2))  & (df['AccessTimeDays'] < (365*3))]\n",
    "print (\"Creating CSV output file showing all filenames with ages at:\", CSV_output_file)\n",
    "df_output_file.to_csv(CSV_output_file)\n",
    "\n",
    "CSV_output_file = CSV_source_file                                                   #start with name of source file\n",
    "if CSV_source_file.lower().endswith(\".csv\"): CSV_output_file = CSV_source_file[:-4]  #remove the last 4 characters (.csv)\n",
    "CSV_output_file += \"_LastAccessed_3to5years.csv\"                                    #append date range to filename\n",
    "df_output_file = df[ (df['AccessTimeDays'] >= (365*3))  & (df['AccessTimeDays'] < (365*5))]\n",
    "print (\"Creating CSV output file showing all filenames with ages at:\", CSV_output_file)\n",
    "df_output_file.to_csv(CSV_output_file)\n",
    "\n",
    "CSV_output_file = CSV_source_file                                                   #start with name of source file\n",
    "if CSV_source_file.lower().endswith(\".csv\"): CSV_output_file = CSV_source_file[:-4]  #remove the last 4 characters (.csv)\n",
    "CSV_output_file += \"_LastAccessed_5to99years.csv\"                                   #append date range to filename\n",
    "df_output_file= df[ (df['AccessTimeDays'] >= (365*5)) & (df['AccessTimeDays'] < (365*99))]\n",
    "print (\"Creating CSV output file showing all filenames with ages at:\", CSV_output_file)\n",
    "df_output_file.to_csv(CSV_output_file)\n",
    "\n",
    "print (\"Finished creating CSV output files\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fb1ca7-1cce-4a4a-98b2-8f70abac2834",
   "metadata": {},
   "source": [
    "# 5 - Create HTML report for presentation to stakeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e40dd23-9d5e-4137-9003-547f92be0787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set filename for HTML report\n",
    "HTML_file = CSV_source_file                                                    #start with name of source file\n",
    "if CSV_source_file.lower().endswith(\".csv\"): HTML_file = CSV_source_file[:-4]  #remove the last 4 characters (.csv)\n",
    "HTML_file += \".html\"                                                           #append .html extension to filename\n",
    "print(f\"Setting HTML output file to {HTML_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c00774-633a-479e-9def-794310bbf57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the current working directory to the folder containing the *.html and *.png files\n",
    "# we want to be in the current directory so the HTML <img src=filename.png> tags do not contain directory paths\n",
    "\n",
    "# figure out the name of the directory containing the HTML_file from the previous step\n",
    "directory_name = os.path.dirname(HTML_file)\n",
    "\n",
    "# set the current directory to the location of the files we are working with\n",
    "os.chdir(directory_name)\n",
    "print(f\"Setting current working directory to {directory_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3c7145-deda-4817-b146-3fedb5ec793f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create HTML header\n",
    "\n",
    "html_header = \"\"\"<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>File Age Report</title>\n",
    "</head>\n",
    "<body>\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e51028d-7796-4d57-a5cf-fb2b9ed4a402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some math to figure out the total number of files in a human-readable format\n",
    "\n",
    "# NOTE: we report files in multiples of 1000 (base 10), but we report bytes in multiples of 1024 (base 2)\n",
    "total_files = num_lines\n",
    "total_Kfiles = round(total_files/1000, 1)\n",
    "total_Mfiles = round(total_files/1000/1000, 1)\n",
    "total_Gfiles = round(total_files/1000/1000/1000, 1)\n",
    "total_Tfiles = round(total_files/1000/1000/1000/1000, 1)\n",
    "total_Pfiles = round(total_files/1000/1000/1000/1000/1000, 1)\n",
    "\n",
    "\n",
    "# Based on the file counts, figure out the preferred unit value (Files, Kfiles, Mfiles, TFiles, PFiles) to display in the report\n",
    "if (total_files  >= 0):  file_units = \"\"  #start with default value nothing\n",
    "if (total_Kfiles >= 1): file_units = \"Thousand\"\n",
    "if (total_Mfiles >= 1): file_units = \"Million\"\n",
    "if (total_Gfiles >= 1): file_units = \"Billion\"\n",
    "if (total_Tfiles >= 1): file_units = \"Trillion\"\n",
    "if (total_Pfiles >= 1): file_units = \"Quadrillion\"\n",
    "print (f\"Based on file counts, the numbers will be reported in {file_units}\")\n",
    "\n",
    " # Based on how big the numbers are, figure out if the vertical y-axis should be in bytes, Mbytes, Gbytes, Tbytes, Pbytes\n",
    "if (file_units == \"\"):            total_files_human_readable = f\"{total_files}\"\n",
    "if (file_units == \"Thousand\"):    total_files_human_readable = f\"{total_Kfiles} Thousand\"\n",
    "if (file_units == \"Million\"):     total_files_human_readable = f\"{total_Mfiles} Million\"\n",
    "if (file_units == \"Billion\"):     total_files_human_readable = f\"{total_Gfiles} Billion\"\n",
    "if (file_units == \"Trillion\"):    total_files_human_readable = f\"{total_Tfiles} Trillion\"\n",
    "if (file_units == \"Quadrillion\"): total_files_human_readable = f\"{total_Pfiles} Quadrillion\"\n",
    "print (f\"Total file count is {total_files_human_readable}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62aff95-ca76-4ca8-a157-b39f05144b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some math to figure out the total disk space consumed in a human-readable format\n",
    "\n",
    "# NOTE: we report files in multiples of 1000 (base 10), but we report bytes in multiples of 1024 (base 2)\n",
    "# bytes are typically too small, convert to more human-readable units, rounded to 1 decimal place\n",
    "total_bytes = sum(df.Bytes)\n",
    "total_Kbytes = round(total_bytes/1024, 1)\n",
    "total_Mbytes = round(total_bytes/1024/1024, 1)\n",
    "total_Gbytes = round(total_bytes/1024/1024/1024, 1)\n",
    "total_Tbytes = round(total_bytes/1024/1024/1024/1024, 1)\n",
    "total_Pbytes = round(total_bytes/1024/1024/1024/1024/1024, 1)\n",
    "\n",
    "\n",
    "# Based on the byte counts, figure out the preferred unit value (Bytes, MegaBytes, GigaBytes, TeraBytes) to display in the report\n",
    "if (total_bytes  >= 0):  byte_units = \"Bytes\"  #start with default value of bytes\n",
    "if (total_Kbytes >= 1): byte_units = \"KiloBytes\"\n",
    "if (total_Mbytes >= 1): byte_units = \"MegaBytes\"\n",
    "if (total_Gbytes >= 1): byte_units = \"GigaBytes\"\n",
    "if (total_Tbytes >= 1): byte_units = \"TeraBytes\"\n",
    "if (total_Pbytes >= 1): byte_units = \"PetaBytes\"\n",
    "print (\"Based on file sizes, the space consumption will be reported in \", byte_units)\n",
    "\n",
    " # Based on how big the numbers are, figure out if the vertical y-axis should be in bytes, Mbytes, Gbytes, Tbytes, Pbytes\n",
    "if (byte_units == \"Bytes\"):     total_bytes_human_readable = f\"{total_bytes} {byte_units}\"\n",
    "if (byte_units == \"KiloBytes\"): total_bytes_human_readable = f\"{total_Kbytes} {byte_units}\"\n",
    "if (byte_units == \"MegaBytes\"): total_bytes_human_readable = f\"{total_Mbytes} {byte_units}\"\n",
    "if (byte_units == \"GigaBytes\"): total_bytes_human_readable = f\"{total_Gbytes} {byte_units}\"\n",
    "if (byte_units == \"TeraBytes\"): total_bytes_human_readable = f\"{total_Tbytes} {byte_units}\"\n",
    "if (byte_units == \"PetaBytes\"): total_bytes_human_readable = f\"{total_Pbytes} {byte_units}\"\n",
    "print (f\"Total space utilization is {total_bytes_human_readable}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81864bce-e3d1-4358-9265-0e43e5df349b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get some detail about the share name, total files, total bytes\n",
    "\n",
    "sample_filename = {df.Filename[0]}\n",
    "total_bytes = sum(df.Bytes)\n",
    "html_body1 = f\"\"\"\n",
    "<br>Total number of bytes in this share: {total_bytes} <b> ({total_bytes_human_readable}) </b>\n",
    "<br>Total number of files in this share: {total_files} <b> ({total_files_human_readable} files) </b>\n",
    "<br>Input source file is {CSV_source_file}\n",
    "<br>Sample filename: {sample_filename}\n",
    "<p><hr>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3c72c4-6d1a-40dc-bd17-8cfbeffc4b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## debugging output \n",
    "html_body1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ad8712-c0f5-441a-8a28-dc5aef7470fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the graphs for the LastModifiedDate\n",
    "\n",
    "# Figure out the filename that was saved earlier\n",
    "graph_filename1 = CSV_source_file                                                    #start with name of source file\n",
    "graph_filename1 = os.path.basename(CSV_source_file)                                  #extract only the filename (remove directory path)\n",
    "if graph_filename1.lower().endswith(\".csv\"): graph_filename1 = graph_filename1[:-4]  #remove the last 4 characters (.csv)\n",
    "graph_filename1 += \"_BarChart\"                                                       #append the type of chart (bar, pie, etc) to the filename\n",
    "graph_filename1 += \"_FileCount\"                                                      #append FileCount|ByteCount to the filename    \n",
    "graph_filename1 += \"_LastModificationDate\"                                           #append LastModificationDate or LastAccessDate to the filename\n",
    "graph_filename1 += \".png\"                                                            #append .png extension to the filename\n",
    "\n",
    "# Figure out the filename that was saved earlier\n",
    "graph_filename2 = CSV_source_file                                                    #start with name of source file\n",
    "graph_filename2 = os.path.basename(CSV_source_file)                                  #extract only the filename (remove directory path)\n",
    "if graph_filename2.lower().endswith(\".csv\"): graph_filename2 = graph_filename2[:-4]  #remove the last 4 characters (.csv)\n",
    "graph_filename2 += \"_PieChart\"                                                       #append the type of chart (bar, pie, etc) to the filename\n",
    "graph_filename2 += \"_FileCount\"                                                      #append FileCount|ByteCount to the filename    \n",
    "graph_filename2 += \"_LastModificationDate\"                                           #append LastModificationDate or LastAccessDate to the filename\n",
    "graph_filename2 += \".png\"                                                            #append .png extension to the filename\n",
    "\n",
    "# Figure out the filename that was saved earlier\n",
    "graph_filename3 = CSV_source_file                                                    #start with name of source file\n",
    "graph_filename3 = os.path.basename(CSV_source_file)                                  #extract only the filename (remove directory path)\n",
    "if graph_filename3.lower().endswith(\".csv\"): graph_filename3 = graph_filename3[:-4]  #remove the last 4 characters (.csv)\n",
    "graph_filename3 += \"_BarChart\"                                                       #append the type of chart (bar, pie, etc) to the filename\n",
    "graph_filename3 += \"_ByteCount\"                                                      #append FileCount|ByteCount to the filename    \n",
    "graph_filename3 += \"_LastModificationDate\"                                           #append LastModificationDate or LastAccessDate to the filename\n",
    "graph_filename3 += \".png\"                                                            #append .png extension to the filename\n",
    "\n",
    "# Figure out the filename that was saved earlier\n",
    "graph_filename4 = CSV_source_file                                                    #start with name of source file\n",
    "graph_filename4 = os.path.basename(CSV_source_file)                                  #extract only the filename (remove directory path)\n",
    "if graph_filename4.lower().endswith(\".csv\"): graph_filename4 = graph_filename4[:-4]  #remove the last 4 characters (.csv)\n",
    "graph_filename4 += \"_PieChart\"                                                       #append the type of chart (bar, pie, etc) to the filename\n",
    "graph_filename4 += \"_ByteCount\"                                                      #append FileCount|ByteCount to the filename    \n",
    "graph_filename4 += \"_LastModificationDate\"                                           #append LastModificationDate or LastAccessDate to the filename\n",
    "graph_filename4 += \".png\"                                                            #append .png extension to the filename\n",
    "\n",
    "\n",
    "html_body2 = f\"\"\"\n",
    "<table border=1>\n",
    "<tr><td colspan=2 bgcolor=lightgray><center><h2>File counts by Last Modification Date</h2></center>\n",
    "<tr><td><img width=\"80%\" src={graph_filename1}>\n",
    "    <td><img width=\"80%\" src={graph_filename2}>\n",
    "<tr><td colspan=2 bgcolor=lightgray><center><h2>Byte counts by Last Modification Date</h2></center>\n",
    "<tr><td><img width=\"80%\" src={graph_filename3}>\n",
    "    <td><img width=\"80%\" src={graph_filename4}>\n",
    "</table>\n",
    "<p><hr>\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f252a712-c7b1-495b-b603-78ce8043c107",
   "metadata": {},
   "outputs": [],
   "source": [
    "## debugging output \n",
    "html_body2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249bfc43-6d2a-4416-a624-74615cd615e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the graphs for the LastAccessDate\n",
    "\n",
    "# Figure out the filename that was saved earlier\n",
    "graph_filename5 = CSV_source_file                                                    #start with name of source file\n",
    "graph_filename5 = os.path.basename(CSV_source_file)                                  #extract only the filename (remove directory path)\n",
    "if graph_filename5.lower().endswith(\".csv\"): graph_filename5 = graph_filename5[:-4]  #remove the last 4 characters (.csv)\n",
    "graph_filename5 += \"_BarChart\"                                                       #append the type of chart (bar, pie, etc) to the filename\n",
    "graph_filename5 += \"_FileCount\"                                                      #append FileCount|ByteCount to the filename    \n",
    "graph_filename5 += \"_LastAccessDate\"                                                 #append LastModificationDate or LastAccessDate to the filename\n",
    "graph_filename5 += \".png\"                                                            #append .png extension to the filename\n",
    "\n",
    "# Figure out the filename that was saved earlier\n",
    "graph_filename6 = CSV_source_file                                                    #start with name of source file\n",
    "graph_filename6 = os.path.basename(CSV_source_file)                                  #extract only the filename (remove directory path)\n",
    "if graph_filename6.lower().endswith(\".csv\"): graph_filename6 = graph_filename6[:-4]  #remove the last 4 characters (.csv)\n",
    "graph_filename6 += \"_PieChart\"                                                       #append the type of chart (bar, pie, etc) to the filename\n",
    "graph_filename6 += \"_FileCount\"                                                      #append FileCount|ByteCount to the filename    \n",
    "graph_filename6 += \"_LastAccessDate\"                                                 #append LastModificationDate or LastAccessDate to the filename\n",
    "graph_filename6 += \".png\"                                                            #append .png extension to the filename\n",
    "\n",
    "# Figure out the filename that was saved earlier\n",
    "graph_filename7 = CSV_source_file                                                    #start with name of source file\n",
    "graph_filename7 = os.path.basename(CSV_source_file)                                  #extract only the filename (remove directory path)\n",
    "if graph_filename7.lower().endswith(\".csv\"): graph_filename7 = graph_filename7[:-4]  #remove the last 4 characters (.csv)\n",
    "graph_filename7 += \"_BarChart\"                                                       #append the type of chart (bar, pie, etc) to the filename\n",
    "graph_filename7 += \"_ByteCount\"                                                      #append FileCount|ByteCount to the filename    \n",
    "graph_filename7 += \"_LastAccessDate\"                                                 #append LastModificationDate or LastAccessDate to the filename\n",
    "graph_filename7 += \".png\"                                                            #append .png extension to the filename\n",
    "\n",
    "# Figure out the filename that was saved earlier\n",
    "graph_filename8 = CSV_source_file                                                    #start with name of source file\n",
    "graph_filename8 = os.path.basename(CSV_source_file)                                  #extract only the filename (remove directory path)\n",
    "if graph_filename8.lower().endswith(\".csv\"): graph_filename8 = graph_filename8[:-4]  #remove the last 4 characters (.csv)\n",
    "graph_filename8 += \"_PieChart\"                                                       #append the type of chart (bar, pie, etc) to the filename\n",
    "graph_filename8 += \"_ByteCount\"                                                      #append FileCount|ByteCount to the filename    \n",
    "graph_filename8 += \"_LastAccessDate\"                                                 #append LastModificationDate or LastAccessDate to the filename\n",
    "graph_filename8 += \".png\"                                                            #append .png extension to the filename\n",
    "\n",
    "html_body3 = f\"\"\"\n",
    "<table border=1>\n",
    "<tr><td colspan=2 bgcolor=lightgray><center><h2>File counts by Last Access Date</h2></center>\n",
    "<tr><td><img  width=\"80%\"src={graph_filename5}>\n",
    "    <td><img  width=\"80%\"src={graph_filename6}>\n",
    "<tr><td colspan=2 bgcolor=lightgray><center><h2>Byte counts by Last Access Date</h2></center>\n",
    "<tr><td><img  width=\"80%\"src={graph_filename7}>\n",
    "    <td><img  width=\"80%\"src={graph_filename8}>\n",
    "</table>\n",
    "<p><hr>\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dd6b97-629c-4751-ab8a-f2f2425ac1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## debugging output \n",
    "html_body3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d722a3-8fa2-401d-a66b-1b4ee2e027c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create HTML fooder\n",
    "html_footer = \"\"\"\n",
    "</body>\n",
    "</html>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1f29d9-6fc3-42e5-8c61-57a2787e57dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# debugging output to confirm the HTML syntax is correct\n",
    "print(f\"{html_header}\")\n",
    "print(f\"{html_body1}\")\n",
    "print(f\"{html_body2}\")\n",
    "print(f\"{html_body3}\")\n",
    "print(f\"{html_footer}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420c2c85-42a0-4b68-bf5f-bae8dd5c148d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the HTML file\n",
    "with open(HTML_file, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(html_header)\n",
    "    file.write(html_body1)\n",
    "    file.write(html_body2)\n",
    "    file.write(html_body3)\n",
    "    file.write(html_footer)\n",
    "\n",
    "print(f\"HTML file '{HTML_file}' has been created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5250ca-aaf0-4f3d-b81e-041538647b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# figure out the name of the directory containing the HTML_file from the previous step\n",
    "directory_name = os.path.dirname(HTML_file)\n",
    "print(f\"Searching for *.html files in {directory_name} directory\")  \n",
    "\n",
    "# Get a list of all HTML files in the current directory (excluding index.html)\n",
    "html_files = [f for f in os.listdir(directory_name) if f.endswith(\".html\") and f != \"index.html\"]\n",
    "print(html_files)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028c906c-55b2-4a33-9d48-a03f7017873c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this point, we have an HTML report for this particular share, but it is common to have lots of shares.\n",
    "# So we will create an index.html file with hyperlinks to all the *.html files in this folder.\n",
    "# The idea here is to make it easy for the end user to just click an index.html file and see reports for all the shares.\n",
    "\n",
    "# import os\n",
    "\n",
    "# figure out the name of the directory containing the HTML_file from the previous step\n",
    "directory_name = os.path.dirname(HTML_file)\n",
    "print(f\"Searching for *.html files in {directory_name} directory\")  \n",
    "\n",
    "# set the current directory to the location of the files we are working with\n",
    "os.chdir(directory_name)\n",
    "\n",
    "# Get a list of all HTML files in the current directory (excluding index.html)\n",
    "html_files = [f for f in os.listdir(directory_name) if f.endswith(\".html\") and f != \"index.html\"]\n",
    "print(html_files)  \n",
    "\n",
    "# Generate the HTML content\n",
    "html_content = \"\"\"<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>File age reports</title>\n",
    "</head>\n",
    "<body>\n",
    "    <h2>File Age Reports for NFS / CIFS shares</h2>\n",
    "    <ul>\n",
    "\"\"\"\n",
    "\n",
    "# Add links to each HTML file\n",
    "for file in html_files:\n",
    "    html_content += f'<br><a href=\"{file}\">{file}</a>\\n'\n",
    "\n",
    "# Close the HTML structure\n",
    "html_content += \"\"\"    </ul>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Save to index.html\n",
    "index_file = \"index.html\"\n",
    "with open(index_file, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(html_content)\n",
    "\n",
    "print(f\"'{index_file}' has been created successfully with links to {len(html_files)} HTML files.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b640ea-fba3-4e04-ae4c-beeafb8ed677",
   "metadata": {},
   "source": [
    "# 6 - Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9757997-7858-41ed-9848-7a9d99590f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make things easy for the user, display the charts that were created earlier\n",
    "# this makes it easy for the user to see all the graphs without having to scroll up and down through the previous cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742a8ee5-9907-4c61-a181-9aa0643dc63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the bar chart for LastModificationDate\n",
    "\n",
    "# Figure out the filename that was saved earlier\n",
    "graph_filename = CSV_source_file                                                    #start with name of source file\n",
    "if CSV_source_file.lower().endswith(\".csv\"): graph_filename = CSV_source_file[:-4]  #remove the last 4 characters (.csv)\n",
    "graph_filename += \"_BarChart\"                                                       #append the type of chart (bar, pie, etc) to the filename\n",
    "graph_filename += \"_FileCount\"                                                      #append FileCount|ByteCount to the filename    \n",
    "graph_filename += \"_LastModificationDate\"                                           #append LastModificationDate or LastAccessDate to the filename\n",
    "graph_filename += \".png\"                                                            #append .png extension to the filename\n",
    "\n",
    "#from IPython.display import display\n",
    "#from PIL import Image\n",
    "\n",
    "# Confirm the image exists\n",
    "if os.path.exists(graph_filename):\n",
    "    print(f\"Confirmed {graph_filename} exists.\")\n",
    "    #\n",
    "    # Open the image\n",
    "    img = Image.open(graph_filename)\n",
    "    #\n",
    "    # Get original dimensions and resize to 33%\n",
    "    original_width, original_height = img.size\n",
    "    new_width = original_width // 3\n",
    "    new_height = original_height // 3\n",
    "    resized_img = img.resize((new_width, new_height))\n",
    "    #\n",
    "    # Display the image\n",
    "    #display(img)\n",
    "    display(resized_img)\n",
    "else:\n",
    "    print(f\"ERROR: cannot find image file {graph_filename}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc33ec3-8434-4056-8c27-f27939a5c914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the pie chart for LastModificationDate\n",
    "\n",
    "# Figure out the filename that was saved earlier\n",
    "graph_filename = CSV_source_file                                                    #start with name of source file\n",
    "if CSV_source_file.lower().endswith(\".csv\"): graph_filename = CSV_source_file[:-4]  #remove the last 4 characters (.csv)\n",
    "graph_filename += \"_PieChart\"                                                       #append the type of chart (bar, pie, etc) to the filename\n",
    "graph_filename += \"_FileCount\"                                                      #append FileCount|ByteCount to the filename    \n",
    "graph_filename += \"_LastModificationDate\"                                           #append LastModificationDate or LastAccessDate to the filename\n",
    "graph_filename += \".png\"                                                            #append .png extension to the filename\n",
    "\n",
    "#from IPython.display import display\n",
    "#from PIL import Image\n",
    "\n",
    "# Confirm the image exists\n",
    "if os.path.exists(graph_filename):\n",
    "    print(f\"Confirmed {graph_filename} exists.\")\n",
    "    #\n",
    "    # Open the image\n",
    "    img = Image.open(graph_filename)\n",
    "    #\n",
    "    # Get original dimensions and resize to 33%\n",
    "    original_width, original_height = img.size\n",
    "    new_width = original_width // 3\n",
    "    new_height = original_height // 3\n",
    "    resized_img = img.resize((new_width, new_height))\n",
    "    #\n",
    "    # Display the image\n",
    "    #display(img)\n",
    "    display(resized_img)\n",
    "else:\n",
    "    print(f\"ERROR: cannot find image file {graph_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce55940-8064-493f-abb3-f87499f7ff73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the bar chart for LastModificationDate\n",
    "\n",
    "# Figure out the filename that was saved earlier\n",
    "graph_filename = CSV_source_file                                                    #start with name of source file\n",
    "if CSV_source_file.lower().endswith(\".csv\"): graph_filename = CSV_source_file[:-4]  #remove the last 4 characters (.csv)\n",
    "graph_filename += \"_BarChart\"                                                       #append the type of chart (bar, pie, etc) to the filename\n",
    "graph_filename += \"_ByteCount\"                                                      #append FileCount|ByteCount to the filename    \n",
    "graph_filename += \"_LastModificationDate\"                                           #append LastModificationDate or LastAccessDate to the filename\n",
    "graph_filename += \".png\"                                                            #append .png extension to the filename\n",
    "\n",
    "#from IPython.display import display\n",
    "#from PIL import Image\n",
    "\n",
    "# Confirm the image exists\n",
    "if os.path.exists(graph_filename):\n",
    "    print(f\"Confirmed {graph_filename} exists.\")\n",
    "    #\n",
    "    # Open the image\n",
    "    img = Image.open(graph_filename)\n",
    "    #\n",
    "    # Get original dimensions and resize to 33%\n",
    "    original_width, original_height = img.size\n",
    "    new_width = original_width // 3\n",
    "    new_height = original_height // 3\n",
    "    resized_img = img.resize((new_width, new_height))\n",
    "    #\n",
    "    # Display the image\n",
    "    #display(img)\n",
    "    display(resized_img)\n",
    "else:\n",
    "    print(f\"ERROR: cannot find image file {graph_filename}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99275700-ee9e-481e-ba4f-98e858b62f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the pie chart for LastModificationDate\n",
    "\n",
    "# Figure out the filename that was saved earlier\n",
    "graph_filename = CSV_source_file                                                    #start with name of source file\n",
    "if CSV_source_file.lower().endswith(\".csv\"): graph_filename = CSV_source_file[:-4]  #remove the last 4 characters (.csv)\n",
    "graph_filename += \"_PieChart\"                                                       #append the type of chart (bar, pie, etc) to the filename\n",
    "graph_filename += \"_ByteCount\"                                                      #append FileCount|ByteCount to the filename    \n",
    "graph_filename += \"_LastModificationDate\"                                           #append LastModificationDate or LastAccessDate to the filename\n",
    "graph_filename += \".png\"                                                            #append .png extension to the filename\n",
    "\n",
    "#from IPython.display import display\n",
    "#from PIL import Image\n",
    "\n",
    "# Confirm the image exists\n",
    "if os.path.exists(graph_filename):\n",
    "    print(f\"Confirmed {graph_filename} exists.\")\n",
    "    #\n",
    "    # Open the image\n",
    "    img = Image.open(graph_filename)\n",
    "    #\n",
    "    # Get original dimensions and resize to 33%\n",
    "    original_width, original_height = img.size\n",
    "    new_width = original_width // 3\n",
    "    new_height = original_height // 3\n",
    "    resized_img = img.resize((new_width, new_height))\n",
    "    #\n",
    "    # Display the image\n",
    "    #display(img)\n",
    "    display(resized_img)\n",
    "else:\n",
    "    print(f\"ERROR: cannot find image file {graph_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898ec4f1-6e98-4ef5-8e0d-858b056fe04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the bar chart for LastAccessDate\n",
    "\n",
    "# Figure out the filename that was saved earlier\n",
    "graph_filename = CSV_source_file                                                    #start with name of source file\n",
    "if CSV_source_file.lower().endswith(\".csv\"): graph_filename = CSV_source_file[:-4]  #remove the last 4 characters (.csv)\n",
    "graph_filename += \"_BarChart\"                                                       #append the type of chart (bar, pie, etc) to the filename\n",
    "graph_filename += \"_FileCount\"                                                      #append FileCount|ByteCount to the filename    \n",
    "graph_filename += \"_LastAccessDate\"                                                 #append LastModificationDate or LastAccessDate to the filename\n",
    "graph_filename += \".png\"                                                            #append .png extension to the filename\n",
    "\n",
    "#from IPython.display import display\n",
    "#from PIL import Image\n",
    "\n",
    "# Confirm the image exists\n",
    "if os.path.exists(graph_filename):\n",
    "    print(f\"Confirmed {graph_filename} exists.\")\n",
    "    #\n",
    "    # Open the image\n",
    "    img = Image.open(graph_filename)\n",
    "    #\n",
    "    # Get original dimensions and resize to 33%\n",
    "    original_width, original_height = img.size\n",
    "    new_width = original_width // 3\n",
    "    new_height = original_height // 3\n",
    "    resized_img = img.resize((new_width, new_height))\n",
    "    #\n",
    "    # Display the image\n",
    "    #display(img)\n",
    "    display(resized_img)\n",
    "else:\n",
    "    print(f\"ERROR: cannot find image file {graph_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fb07b3-bc58-4cb3-9b28-317bb61ed2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the pie chart for LastAccessDate\n",
    "\n",
    "# Figure out the filename that was saved earlier\n",
    "graph_filename = CSV_source_file                                                    #start with name of source file\n",
    "if CSV_source_file.lower().endswith(\".csv\"): graph_filename = CSV_source_file[:-4]  #remove the last 4 characters (.csv)\n",
    "graph_filename += \"_PieChart\"                                                       #append the type of chart (bar, pie, etc) to the filename\n",
    "graph_filename += \"_FileCount\"                                                      #append FileCount|ByteCount to the filename    \n",
    "graph_filename += \"_LastAccessDate\"                                                 #append LastModificationDate or LastAccessDate to the filename\n",
    "graph_filename += \".png\"                                                            #append .png extension to the filename\n",
    "\n",
    "#from IPython.display import display\n",
    "#from PIL import Image\n",
    "\n",
    "# Confirm the image exists\n",
    "if os.path.exists(graph_filename):\n",
    "    print(f\"Confirmed {graph_filename} exists.\")\n",
    "    #\n",
    "    # Open the image\n",
    "    img = Image.open(graph_filename)\n",
    "    #\n",
    "    # Get original dimensions and resize to 33%\n",
    "    original_width, original_height = img.size\n",
    "    new_width = original_width // 3\n",
    "    new_height = original_height // 3\n",
    "    resized_img = img.resize((new_width, new_height))\n",
    "    #\n",
    "    # Display the image\n",
    "    #display(img)\n",
    "    display(resized_img)\n",
    "else:\n",
    "    print(f\"ERROR: cannot find image file {graph_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0d424c-56c1-4557-976c-17104a58b586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the bar chart for LastAccessDate\n",
    "\n",
    "# Figure out the filename that was saved earlier\n",
    "graph_filename = CSV_source_file                                                    #start with name of source file\n",
    "if CSV_source_file.lower().endswith(\".csv\"): graph_filename = CSV_source_file[:-4]  #remove the last 4 characters (.csv)\n",
    "graph_filename += \"_BarChart\"                                                       #append the type of chart (bar, pie, etc) to the filename\n",
    "graph_filename += \"_ByteCount\"                                                      #append FileCount|ByteCount to the filename    \n",
    "graph_filename += \"_LastAccessDate\"                                                 #append LastModificationDate or LastAccessDate to the filename\n",
    "graph_filename += \".png\"                                                            #append .png extension to the filename\n",
    "\n",
    "#from IPython.display import display\n",
    "#from PIL import Image\n",
    "\n",
    "# Confirm the image exists\n",
    "if os.path.exists(graph_filename):\n",
    "    print(f\"Confirmed {graph_filename} exists.\")\n",
    "    #\n",
    "    # Open the image\n",
    "    img = Image.open(graph_filename)\n",
    "    #\n",
    "    # Get original dimensions and resize to 33%\n",
    "    original_width, original_height = img.size\n",
    "    new_width = original_width // 3\n",
    "    new_height = original_height // 3\n",
    "    resized_img = img.resize((new_width, new_height))\n",
    "    #\n",
    "    # Display the image\n",
    "    #display(img)\n",
    "    display(resized_img)\n",
    "else:\n",
    "    print(f\"ERROR: cannot find image file {graph_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f34d519-24ae-4691-9867-fb96adea1b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the pie chart for LastAccessDate\n",
    "\n",
    "# Figure out the filename that was saved earlier\n",
    "graph_filename = CSV_source_file                                                    #start with name of source file\n",
    "if CSV_source_file.lower().endswith(\".csv\"): graph_filename = CSV_source_file[:-4]  #remove the last 4 characters (.csv)\n",
    "graph_filename += \"_PieChart\"                                                       #append the type of chart (bar, pie, etc) to the filename\n",
    "graph_filename += \"_ByteCount\"                                                      #append FileCount|ByteCount to the filename    \n",
    "graph_filename += \"_LastAccessDate\"                                                 #append LastModificationDate or LastAccessDate to the filename\n",
    "graph_filename += \".png\"                                                            #append .png extension to the filename\n",
    "\n",
    "#from IPython.display import display\n",
    "#from PIL import Image\n",
    "\n",
    "# Confirm the image exists\n",
    "if os.path.exists(graph_filename):\n",
    "    print(f\"Confirmed {graph_filename} exists.\")\n",
    "    #\n",
    "    # Open the image\n",
    "    img = Image.open(graph_filename)\n",
    "    #\n",
    "    # Get original dimensions and resize to 33%\n",
    "    original_width, original_height = img.size\n",
    "    new_width = original_width // 3\n",
    "    new_height = original_height // 3\n",
    "    resized_img = img.resize((new_width, new_height))\n",
    "    #\n",
    "    # Display the image\n",
    "    #display(img)\n",
    "    display(resized_img)\n",
    "else:\n",
    "    print(f\"ERROR: cannot find image file {graph_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02997a7-2d16-496a-8f96-8d11dd992f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the total number of files and used bytes (by last modification date)\n",
    "print(f\"Total number of files by Last Modification Date:\", len(df))\n",
    "\n",
    "if (y_axis_units == 'Bytes'):\n",
    "    total_bytes = LastModified_bytes0to90days  + LastModified_bytes90to180days  + LastModified_bytes180to365days  + LastModified_bytes1to2years  + LastModified_bytes2to3years  + LastModified_bytes3to5years  + LastModified_bytes5to99years\n",
    "if (y_axis_units == 'KiloBytes'):\n",
    "    total_bytes = LastModified_Kbytes0to90days + LastModified_Kbytes90to180days + LastModified_Kbytes180to365days + LastModified_Kbytes1to2years + LastModified_Kbytes2to3years + LastModified_Kbytes3to5years + LastModified_Kbytes5to99years\n",
    "if (y_axis_units == 'MegaBytes'):\n",
    "    total_bytes = LastModified_Mbytes0to90days + LastModified_Mbytes90to180days + LastModified_Mbytes180to365days + LastModified_Mbytes1to2years + LastModified_Mbytes2to3years + LastModified_Mbytes3to5years + LastModified_Mbytes5to99years\n",
    "if (y_axis_units == 'GigaBytes'):\n",
    "    total_bytes = LastModified_Gbytes0to90days + LastModified_Gbytes90to180days + LastModified_Gbytes180to365days + LastModified_Gbytes1to2years + LastModified_Gbytes2to3years + LastModified_Gbytes3to5years + LastModified_Gbytes5to99years\n",
    "if (y_axis_units == 'TeraBytes'):\n",
    "    total_bytes = LastModified_Tbytes0to90days + LastModified_Tbytes90to180days + LastModified_Tbytes180to365days + LastModified_Tbytes1to2years + LastModified_Tbytes2to3years + LastModified_Tbytes3to5years + LastModified_Tbytes5to99years\n",
    "if (y_axis_units == 'PetaBytes'):\n",
    "    total_bytes = LastModified_Pbytes0to90days + LastModified_Pbytes90to180days + LastModified_Pbytes180to365days + LastModified_Pbytes1to2years + LastModified_Pbytes2to3years + LastModified_Pbytes3to5years + LastModified_Pbytes5to99years\n",
    "print(f\"Total space consumed:\", total_bytes, y_axis_units)\n",
    "print(f\" \")\n",
    "\n",
    "\n",
    "\n",
    "# show the total number of files and used bytes (by last access date)\n",
    "print(f\"Total number of files by Last Access Date:\", len(df))\n",
    "\n",
    "if (y_axis_units == 'Bytes'):\n",
    "    total_bytes = LastAccessed_bytes0to90days  + LastAccessed_bytes90to180days  + LastAccessed_bytes180to365days  + LastAccessed_bytes1to2years  + LastAccessed_bytes2to3years  + LastAccessed_bytes3to5years  + LastAccessed_bytes5to99years\n",
    "if (y_axis_units == 'KiloBytes'):\n",
    "    total_bytes = LastAccessed_Kbytes0to90days + LastAccessed_Kbytes90to180days + LastAccessed_Kbytes180to365days + LastAccessed_Kbytes1to2years + LastAccessed_Kbytes2to3years + LastAccessed_Kbytes3to5years + LastAccessed_Kbytes5to99years\n",
    "if (y_axis_units == 'MegaBytes'):\n",
    "    total_bytes = LastAccessed_Mbytes0to90days + LastAccessed_Mbytes90to180days + LastAccessed_Mbytes180to365days + LastAccessed_Mbytes1to2years + LastAccessed_Mbytes2to3years + LastAccessed_Mbytes3to5years + LastAccessed_Mbytes5to99years\n",
    "if (y_axis_units == 'GigaBytes'):\n",
    "    total_bytes = LastAccessed_Gbytes0to90days + LastAccessed_Gbytes90to180days + LastAccessed_Gbytes180to365days + LastAccessed_Gbytes1to2years + LastAccessed_Gbytes2to3years + LastAccessed_Gbytes3to5years + LastAccessed_Gbytes5to99years\n",
    "if (y_axis_units == 'TeraBytes'):\n",
    "    total_bytes = LastAccessed_Tbytes0to90days + LastAccessed_Tbytes90to180days + LastAccessed_Tbytes180to365days + LastAccessed_Tbytes1to2years + LastAccessed_Tbytes2to3years + LastAccessed_Tbytes3to5years + LastAccessed_Tbytes5to99years\n",
    "if (y_axis_units == 'PetaBytes'):\n",
    "    total_bytes = LastAccessed_Pbytes0to90days + LastAccessed_Pbytes90to180days + LastAccessed_Pbytes180to365days + LastAccessed_Pbytes1to2years + LastAccessed_Pbytes2to3years + LastAccessed_Pbytes3to5years + LastAccessed_Pbytes5to99years\n",
    "print(f\"Total space consumed:\", total_bytes, y_axis_units)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfca88db-c8ce-4526-b339-7207d3d5703e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45abdebb-1b09-475f-b09a-27c7d08c6346",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
